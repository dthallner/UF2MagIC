{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bfbea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcd4ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start new Magic contribution for locality\n",
    "\n",
    "sites_dict = {'site': [], 'method_codes': [], 'citations': []}\n",
    "samples_dict = {'site': [], 'sample': [],'method_codes': [], 'citations': [],\n",
    "                'azimuth': [], 'dip': [], 'bed_dip_direction': [], 'bed_dip': [],'result_quality': []}\n",
    "specimens_dict = {'specimen': [], 'sample': [],'method_codes': [],'experiments': [],'volume': [], 'citations': [],'result_quality':[]}\n",
    "measurement_dict = {'specimen': [],'method_codes': [],'dir_dec': [],'dir_inc': [],'magn_volume': [], 'experiment': [],\n",
    "                    'dir_csd': [],'treat_temp':[], 'meas_temp': [],'treat_ac_field': [], 'treat_dc_field': [],'citations': [],\n",
    "                     'treat_dc_field_phi': [],'instrument_codes': [],'sequence':[], 'magn_moment': [], 'measurement': [],\n",
    "                    'treat_dc_field_theta': [],'treat_step_num':[],'quality': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48bccc",
   "metadata": {},
   "source": [
    "### Cryo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b728e36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find orientation data for  MAT7-5.1\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# run this cell once for each file that you want to have in a location\n",
    "# when done, skip to the very last cell that writes magic files\n",
    "# .dat and .az files have to be in the same directory!! the name of the .az file has to agree with the one specified in the .dat file!\n",
    "\n",
    "file_path = r'G:\\My Drive\\__Postdoc\\Misc\\Side_quests\\UF2MagIC/22Deccan_Thermal-Master.dat' #path + file name\n",
    "\n",
    "treatment = 'both' # 'THD', 'AFD', or both' for directions; 'IZZI' for izzi pints (more to be added when needed)\n",
    "min_THD_step = 100 # only used if treatment = 'both'\n",
    "\n",
    "DC_field = 60 # uT value for the used lab field in intensity experiments\n",
    "DC_phi = 0 # 'declination' of applied field \n",
    "DC_theta = 90 # 'inclination' of applied field \n",
    "\n",
    "site_delimiter = '-' # e.g., '-' for MAT1-1.2 ([site]-[sample].[specimen]); if there is none, use ''\n",
    "site_chars = 3 # number of characters at the beginning of the sample name that are the site identifier. only used if \n",
    "               # site_delimiter = ''\n",
    "specimen_delimiter = '' # e.g., '.' for MAT1-1.2 ([site]-[sample].[specimen]); if specimen = sample, use ''\n",
    "specimen_chars = 0 # number of characters from the end of the sample names if there is no delimiter (use 0 for spec = sample)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "f = open(file_path)\n",
    "data = f.read().splitlines()\n",
    "f.close\n",
    "temp_data, specs = [],[]\n",
    "if True: # remove line\n",
    "    for i in range(len(data)):\n",
    "        if i <= 4:\n",
    "            temp_data.append(data[i])\n",
    "        else:\n",
    "            line = data[i].split()\n",
    "            if line[0] in specs: continue\n",
    "            specs.append(line[0])\n",
    "            temp_data.append(data[i])\n",
    "            for j in range(i+1,len(data)):\n",
    "                line = data[j].split()\n",
    "                if line[0] == specs[-1]: \n",
    "                    temp_data.append(data[j])\n",
    "        #print(temp_data[-1])\n",
    "    if len(temp_data) == len(data):\n",
    "        data = temp_data\n",
    "    else: \n",
    "        print('Measurement data could not be sorted')\n",
    "        print(len(data),len(temp_data))\n",
    "                \n",
    "\n",
    "orientations = {'sample': [], 'azi': [], 'plunge': [], 'bed_dir': [], 'bed_dip': []}\n",
    "sequence, last_treatment = 0, -1\n",
    "for ii in range(len(data)):\n",
    "    line = data[ii].split()\n",
    "    if ii == 0:\n",
    "        f = open(file_path.strip(line[0])+line[-1])\n",
    "        orientations_raw = f.readlines()\n",
    "        f.close()\n",
    "        for o_line in orientations_raw:\n",
    "            o_line = o_line.split()\n",
    "            orientations['sample'].append(o_line[0])\n",
    "            orientations['azi'].append(o_line[1])\n",
    "            orientations['plunge'].append(o_line[2])\n",
    "            orientations['bed_dir'].append(o_line[3])\n",
    "            orientations['bed_dip'].append(o_line[4])\n",
    "        continue\n",
    "    elif ii < 5: continue\n",
    "    \n",
    "    if not site_delimiter == '':\n",
    "        site_name = line[0].split(site_delimiter)[0]\n",
    "    else:\n",
    "        site_name = line[0][:site_chars+1]\n",
    "    duplicate_site = False\n",
    "    if not site_name in sites_dict['site']: sites_dict['site'].append(site_name)\n",
    "    else: duplicate_site = True\n",
    "    if not duplicate_site: sites_dict['citations'].append('This study')\n",
    "    if treatment == 'THD': \n",
    "        if not duplicate_site: \n",
    "            sites_dict['method_codes'].append('LP-DIR-T')\n",
    "        else: \n",
    "            for j in range(len(sites_dict['site'])):\n",
    "                if sites_dict['site'] == site_name:\n",
    "                    if not 'LP-DIR-T' in sites_dict['method_codes'][j]:\n",
    "                        sites_dict['method_codes'][j] += ':LP-DIR-T'\n",
    "    elif treatment == 'AFD': \n",
    "        if not duplicate_site: \n",
    "            sites_dict['method_codes'].append('LP-DIR-AF')\n",
    "        else: \n",
    "            for j in range(len(sites_dict['site'])):\n",
    "                if sites_dict['site'] == site_name:\n",
    "                    if not 'LP-DIR-AF' in sites_dict['method_codes'][j]:\n",
    "                        sites_dict['method_codes'][j] += ':LP-DIR-AF'\n",
    "    elif treatment == 'both': \n",
    "        if not duplicate_site: \n",
    "            sites_dict['method_codes'].append('LP-DIR-AF:LP-DIR-T')\n",
    "        else: \n",
    "            for j in range(len(sites_dict['site'])):\n",
    "                if sites_dict['site'] == site_name:\n",
    "                    if not 'LP-DIR-AF' in sites_dict['method_codes'][j]:\n",
    "                        sites_dict['method_codes'][j] += ':LP-DIR-AF'\n",
    "                    if not 'LP-DIR-T' in sites_dict['method_codes'][j]:\n",
    "                        sites_dict['method_codes'][j] += ':LP-DIR-T'\n",
    "    elif treatment == 'IZZI': \n",
    "        if not duplicate_site: \n",
    "            sites_dict['method_codes'].append('LP-PI-TRM:LP-PI-BT-IZZI:LP-PI-ALT-PTRM')\n",
    "        else: \n",
    "            for j in range(len(sites_dict['site'])):\n",
    "                if sites_dict['site'] == site_name:\n",
    "                    if not 'LP-PI-BT-IZZI' in sites_dict['method_codes'][j]:\n",
    "                        sites_dict['method_codes'][j] += ':LP-PI-TRM:LP-PI-BT-IZZI:LP-PI-ALT-PTRM'\n",
    "    else: print('Currently, only THD (thermal demag - directions), AFD (AF demag - directions), both (combined AF+TH) and IZZI (izzi pint) are availabe options')\n",
    "    \n",
    "    name = line[0]\n",
    "    if not specimen_delimiter == '':      \n",
    "        sample_name = name.split(specimen_delimiter)[0]\n",
    "        specimen_name = name\n",
    "    else:       \n",
    "        if not specimen_chars == 0:\n",
    "            sample_name = name[:-specimen_chars]\n",
    "        else:\n",
    "            sample_name = name\n",
    "        specimen_name = name\n",
    "    \n",
    "    duplicate_sample = False\n",
    "    if not sample_name in samples_dict['sample']: samples_dict['sample'].append(sample_name)\n",
    "    else: duplicate_sample = True\n",
    "        \n",
    "    if not duplicate_sample: \n",
    "        samples_dict['site'].append(site_name)\n",
    "        samples_dict['citations'].append('This study')\n",
    "        samples_dict['result_quality'].append('g')\n",
    "    \n",
    "    duplicate_specimen = False\n",
    "    if not name in specimens_dict['specimen']:\n",
    "        specimens_dict['specimen'].append(name)\n",
    "        specimens_dict['sample'].append(sample_name)\n",
    "        specimens_dict['citations'].append('This study')\n",
    "        specimens_dict['result_quality'].append('g')\n",
    "    else: duplicate_specimen = True\n",
    "    \n",
    "    if treatment == 'THD': \n",
    "        if not duplicate_sample: samples_dict['method_codes'].append('LP-DIR-T')\n",
    "        else:\n",
    "            for j in range(len(samples_dict['sample'])):\n",
    "                if samples_dict['sample'][j] == sample_name:\n",
    "                    if not 'LP-DIR-T' in samples_dict['method_codes'][j]:\n",
    "                        samples_dict['method_codes'][j] += ':LP-DIR-T'\n",
    "        if not duplicate_specimen:\n",
    "            specimens_dict['method_codes'].append('LP-DIR-T')\n",
    "            specimens_dict['experiments'].append(name+'_'+'LP-DIR-T')\n",
    "    elif treatment == 'AFD': \n",
    "        if not duplicate_sample: samples_dict['method_codes'].append('LP-DIR-AF')\n",
    "        else:\n",
    "            for j in range(len(samples_dict['sample'])):\n",
    "                if samples_dict['sample'][j] == sample_name:\n",
    "                    if not 'LP-DIR-AF' in samples_dict['method_codes'][j]:\n",
    "                        samples_dict['method_codes'][j] += ':LP-DIR-AF'\n",
    "        if not duplicate_specimen:\n",
    "            specimens_dict['method_codes'].append('LP-DIR-AF')\n",
    "            specimens_dict['experiments'].append(name+'_'+'LP-DIR-AF')\n",
    "    elif treatment == 'both': \n",
    "        if not duplicate_sample: samples_dict['method_codes'].append('LP-DIR-AF:LP-DIR-T')\n",
    "        else:\n",
    "            for j in range(len(samples_dict['sample'])):\n",
    "                if samples_dict['sample'][j] == sample_name:\n",
    "                    if not 'LP-DIR-T' in samples_dict['method_codes'][j] and 'LP-DIR-AF' in samples_dict['method_codes'][j]:\n",
    "                        samples_dict['method_codes'][j] += ':LP-DIR-T'\n",
    "                    elif not 'LP-DIR-AF' in samples_dict['method_codes'][j] and 'LP-DIR-T' in samples_dict['method_codes'][j]:\n",
    "                        samples_dict['method_codes'][j] += ':LP-DIR-AF'\n",
    "                    elif not 'LP-DIR-AF:LP-DIR-T' in samples_dict['method_codes'][j] and  not 'LP-DIR-T:LP-DIR-AF' in samples_dict['method_codes'][j]:\n",
    "                        print(sample_name, samples_dict['method_codes'][j])\n",
    "                        samples_dict['method_codes'][j] += ':LP-DIR-AF:LP-DIR-T'\n",
    "                    break\n",
    "        if not duplicate_specimen:\n",
    "            specimens_dict['method_codes'].append('LP-DIR-AF:LP-DIR-T')\n",
    "            specimens_dict['experiments'].append(name+'_'+'LP-DIR-AFT')\n",
    "        \n",
    "    elif treatment == 'IZZI':\n",
    "        if not duplicate_sample: samples_dict['method_codes'].append('LP-PI-TRM:LP-PI-BT-IZZI:LP-PI-ALT-PTRM')\n",
    "        else:\n",
    "            for j in range(len(samples_dict['sample'])):\n",
    "                if samples_dict['sample'][j] == sample_name:\n",
    "                    if not 'LP-PI-BT-IZZI' in samples_dict['method_codes'][j]:\n",
    "                        samples_dict['method_codes'][j] += ':LP-PI-TRM:LP-PI-BT-IZZI:LP-PI-ALT-PTRM'\n",
    "        if not duplicate_specimen:\n",
    "            specimens_dict['method_codes'].append('LP-PI-TRM:LP-PI-BT-IZZI:LP-PI-ALT-PTRM')\n",
    "            specimens_dict['experiments'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not duplicate_sample:\n",
    "        for jj in range(len(orientations['sample'])):\n",
    "            if orientations['sample'][jj] == name:\n",
    "                samples_dict['azimuth'].append(str(orientations['azi'][jj]))\n",
    "                samples_dict['dip'].append(str((float(orientations['plunge'][jj])-90)))\n",
    "\n",
    "                samples_dict['bed_dip_direction'].append(str(orientations['bed_dir'][jj]))\n",
    "                samples_dict['bed_dip'].append(str(orientations['bed_dip'][jj]))\n",
    "                break\n",
    "        if not len(samples_dict['sample']) == len(samples_dict['bed_dip']):\n",
    "            print('Could not find orientation data for ', name)\n",
    "            samples_dict['azimuth'].append('')\n",
    "            samples_dict['dip'].append('') #check orientation\n",
    "            samples_dict['bed_dip_direction'].append('')\n",
    "            samples_dict['bed_dip'].append('')\n",
    "    \n",
    "    treat = float(line[1])\n",
    "    volume = 10 * 10**-6    #  10 cc assumed by cryo to m2                                     \n",
    "    if not duplicate_specimen: specimens_dict['volume'].append(str(volume)) # m^3\n",
    "      \n",
    "    measurement_dict['specimen'].append(name)\n",
    "    \n",
    "    if treat > last_treatment and not treatment == 'IZZI': sequence += 1\n",
    "    elif treatment == 'IZZI': \n",
    "        sequence += 1\n",
    "        if sequence > 1 and not measurement_dict['specimen'][-1] == measurement_dict['specimen'][-2]: sequence = 0\n",
    "    if sequence < 10: seq_str = '0'+str(sequence)\n",
    "    else: seq_str = str(sequence)\n",
    "    measurement_dict['sequence'].append(str(seq_str))\n",
    "    measurement_dict['citations'].append('This study')\n",
    "    \n",
    "    if sequence == 1: measurement_dict['quality'].append('g')\n",
    "    else:\n",
    "        if not treatment == 'IZZI':\n",
    "            if treat == last_treatment: # attempting to catch repeat measurements\n",
    "                #print(treat,last_treatment)\n",
    "                measurement_dict['quality'][-1] = 'b' # assumes that the first one was bad\n",
    "        measurement_dict['quality'].append('g')\n",
    "                                             \n",
    "    if treatment == 'THD':\n",
    "        measurement_dict['treat_temp'].append(str(float(treat)+273)) # K\n",
    "        measurement_dict['meas_temp'].append(str(273)) # K\n",
    "        measurement_dict['treat_ac_field'].append(str(0)) # T\n",
    "        measurement_dict['treat_dc_field'].append(str(0)) # T\n",
    "        measurement_dict['treat_dc_field_phi'].append(str(0)) # deg\n",
    "        measurement_dict['treat_dc_field_theta'].append(str(0)) # deg\n",
    "        measurement_dict['method_codes'].append('LT-T-Z')\n",
    "        measurement_dict['experiment'].append(name+'_'+'LP-DIR-T')\n",
    "        measurement_dict['measurement'].append(name+'_'+'LP-DIR-T'+'-'+str(seq_str))\n",
    "                    \n",
    "    elif treatment == 'AFD':\n",
    "        measurement_dict['treat_temp'].append(str(273)) # K\n",
    "        measurement_dict['meas_temp'].append(str(273)) # K\n",
    "        measurement_dict['treat_ac_field'].append(str(float(treat)/10**3)) # T\n",
    "        measurement_dict['treat_dc_field'].append(str(0)) # T\n",
    "        measurement_dict['treat_dc_field_phi'].append(str(0)) # deg\n",
    "        measurement_dict['treat_dc_field_theta'].append(str(0)) # deg\n",
    "        measurement_dict['method_codes'].append('LT-AF-Z')\n",
    "        measurement_dict['experiment'].append(name+'_'+'LP-DIR-AF')\n",
    "        measurement_dict['measurement'].append(name+'_'+'LP-DIR-AF'+'-'+str(seq_str))\n",
    "    \n",
    "    elif treatment == 'both':\n",
    "        if treat < min_THD_step:\n",
    "            measurement_dict['treat_temp'].append(str(273)) # K\n",
    "            measurement_dict['meas_temp'].append(str(273)) # K\n",
    "            measurement_dict['treat_ac_field'].append(str(float(treat)/10**3)) # T\n",
    "            measurement_dict['treat_dc_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_dc_field_phi'].append(str(0)) # deg\n",
    "            measurement_dict['treat_dc_field_theta'].append(str(0)) # deg\n",
    "            measurement_dict['method_codes'].append('LT-AF-Z')\n",
    "            measurement_dict['experiment'].append(name+'_'+'LP-DIR-AFT')\n",
    "            measurement_dict['measurement'].append(name+'_'+'LP-DIR-AFT'+'-'+str(seq_str))\n",
    "        else:\n",
    "            measurement_dict['treat_temp'].append(str(float(treat)+273)) # K\n",
    "            measurement_dict['meas_temp'].append(str(273)) # K\n",
    "            measurement_dict['treat_ac_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_dc_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_dc_field_phi'].append(str(0)) # deg\n",
    "            measurement_dict['treat_dc_field_theta'].append(str(0)) # deg\n",
    "            measurement_dict['method_codes'].append('LT-T-Z')\n",
    "            measurement_dict['experiment'].append(name+'_'+'LP-DIR-AFT')\n",
    "            measurement_dict['measurement'].append(name+'_'+'LP-DIR-AFT'+'-'+str(seq_str))\n",
    "            \n",
    "    if treatment == 'IZZI':\n",
    "        if str(treat)[-2:] == '.0': # Z\n",
    "            measurement_dict['treat_temp'].append(str(float(str(treat)[:-2])+273)) # K\n",
    "            measurement_dict['meas_temp'].append(str(273)) # K\n",
    "            measurement_dict['treat_dc_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_ac_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_dc_field_phi'].append(str(0)) # deg\n",
    "            measurement_dict['treat_dc_field_theta'].append(str(0)) # deg\n",
    "            if treat == 0:\n",
    "                measurement_dict['method_codes'].append('LT-NO:LP-PI-TRM:LP-PI-ALT-PTRM:LP-PI-BT-IZZI')\n",
    "            else:\n",
    "                if 'LT-NO' in measurement_dict['method_codes'][-1] or 'LT-PTRM-I' in measurement_dict['method_codes'][-1] \\\n",
    "                or ('LT-T-Z' in measurement_dict['method_codes'][-1] and 'LP-PI-TRM-IZ' in measurement_dict['method_codes'][-1]):\n",
    "                    measurement_dict['method_codes'].append('LT-T-Z:LP-PI-TRM-ZI:LP-PI-TRM:LP-PI-ALT-PTRM:LP-PI-BT-IZZI')\n",
    "                else:\n",
    "                    measurement_dict['method_codes'].append('LT-T-Z:LP-PI-TRM-IZ:LP-PI-TRM:LP-PI-ALT-PTRM:LP-PI-BT-IZZI')\n",
    "            measurement_dict['experiment'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI')\n",
    "            measurement_dict['measurement'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI'+'-'+str(seq_str))\n",
    "        elif str(treat)[-2:] == '.1': # I\n",
    "            measurement_dict['treat_temp'].append(str(float(str(treat)[:-2])+273)) # K\n",
    "            measurement_dict['meas_temp'].append(str(273)) # K\n",
    "            measurement_dict['treat_dc_field'].append(str(DC_field*10**-6)) # T\n",
    "            measurement_dict['treat_ac_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_dc_field_phi'].append(str(0)) # deg\n",
    "            measurement_dict['treat_dc_field_theta'].append(str(0)) # deg\n",
    "            if 'LT-PTRM-I' in measurement_dict['method_codes'][-1] \\\n",
    "                or ('LT-T-Z' in measurement_dict['method_codes'][-1] and 'LP-PI-TRM-ZI' in measurement_dict['method_codes'][-1]):\n",
    "                measurement_dict['method_codes'].append('LT-T-I:LP-PI-TRM-ZI:LP-PI-TRM:LP-PI-ALT-PTRM:LP-PI-BT-IZZI')\n",
    "            else:\n",
    "                measurement_dict['method_codes'].append('LT-T-I:LP-PI-TRM-IZ:LP-PI-TRM:LP-PI-ALT-PTRM:LP-PI-BT-IZZI')\n",
    "            measurement_dict['experiment'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI')\n",
    "            measurement_dict['measurement'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI'+'-'+str(seq_str))\n",
    "        elif str(treat)[-2:] == '.2': # P\n",
    "            measurement_dict['treat_temp'].append(str(float(str(treat)[:-2])+273)) # K\n",
    "            measurement_dict['meas_temp'].append(str(273)) # K\n",
    "            measurement_dict['treat_dc_field'].append(str(DC_field*10**-6)) # T\n",
    "            measurement_dict['treat_ac_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_dc_field_phi'].append(str(0)) # deg\n",
    "            measurement_dict['treat_dc_field_theta'].append(str(0)) # deg\n",
    "            measurement_dict['method_codes'].append('LT-PTRM-I:LP-PI-TRM:LP-PI-ALT-PTRM:LP-PI-BT-IZZI')\n",
    "            measurement_dict['experiment'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI')\n",
    "            measurement_dict['measurement'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI'+'-'+str(seq_str))\n",
    "        elif str(treat)[-2:] == '.3': # T\n",
    "            measurement_dict['treat_temp'].append(str(float(str(treat)[:-2])+273)) # K\n",
    "            measurement_dict['meas_temp'].append(str(273)) # K\n",
    "            measurement_dict['treat_dc_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_ac_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_dc_field_phi'].append(str(0)) # deg\n",
    "            measurement_dict['treat_dc_field_theta'].append(str(0)) # deg\n",
    "            measurement_dict['method_codes'].append('LT-PTRM-Z:LP-PI-TRM:LP-PI-ALT-PTRM:LP-PI-BT-IZZI')\n",
    "            measurement_dict['experiment'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI')\n",
    "            measurement_dict['measurement'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI'+'-'+str(seq_str))\n",
    "        elif str(treat)[-2:] == '.4': # A\n",
    "            measurement_dict['treat_temp'].append(str(float(str(treat)[:-2])+273)) # K\n",
    "            measurement_dict['meas_temp'].append(str(273)) # K\n",
    "            measurement_dict['treat_dc_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_ac_field'].append(str(0)) # T\n",
    "            measurement_dict['treat_dc_field_phi'].append(str(0)) # deg\n",
    "            measurement_dict['treat_dc_field_theta'].append(str(0)) # deg\n",
    "            measurement_dict['method_codes'].append('LT-PTRM-AC:LP-PI-TRM:LP-PI-ALT-PTRM:LP-PI-BT-IZZI')\n",
    "            measurement_dict['experiment'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI')\n",
    "            measurement_dict['measurement'].append(name+'_'+'LP-PI-TRM_LP-PI-ALT-PTRM_LP-PI-BT-IZZI'+'-'+str(seq_str))\n",
    "        \n",
    "    intensity = float(line[4])*25/np.pi # Oe => Oe * 20/pi = A/m \n",
    "    measurement_dict['magn_volume'].append(str(float(intensity))) # A/m\n",
    "    measurement_dict['magn_moment'].append(str((float(intensity))*(float(volume))))#/10**6))) # Am^2\n",
    "    GDec = (line[7]) # not needed for demag GUI\n",
    "    GInc = (line[8]) # not needed for demag GUI\n",
    "    csd = line[3] # not sure if this is actually csd or something else\n",
    "    measurement_dict['dir_csd'].append(csd)\n",
    "    CDec = line[5]\n",
    "    measurement_dict['dir_dec'].append(CDec)\n",
    "    CInc = line[6]\n",
    "    measurement_dict['dir_inc'].append(CInc)\n",
    "    measurement_dict['instrument_codes'].append('Cryo')\n",
    "    measurement_dict['treat_step_num'].append(str(int(seq_str)+1))\n",
    "    last_treatment = treat\n",
    "    \n",
    "print('Done!')\n",
    "\n",
    "#for key in samples_dict.keys():\n",
    "#    print(key,len(samples_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80aea2",
   "metadata": {},
   "source": [
    "### Molspin data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "281f2950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specimens for site I1938 added successfully\n"
     ]
    }
   ],
   "source": [
    "# add measurement data for a site \n",
    "# run this cell once for each directory specified below. Use one directory per site & experiment\n",
    "#assumes that specimen == sample\n",
    "\n",
    "site_name = 'I1938' # Specify name of site\n",
    "treatment = 'THD' # Specify if the experiment was thermal demag ('THD') or AF demag ('AFD')\n",
    "site_dir_path = r'G:\\My Drive\\__Postdoc\\Misc\\Side_quests\\UF2MagIC\\I1938' # absolute or relative path to directory \n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "duplicate_site = False\n",
    "if not site_name in sites_dict['site']: sites_dict['site'].append(site_name)\n",
    "else: duplicate_site = True\n",
    "if not duplicate_site: sites_dict['citations'].append('This study')\n",
    "if treatment == 'THD': \n",
    "    if not duplicate_site: \n",
    "        sites_dict['method_codes'].append('LP-DIR-T')\n",
    "    else: \n",
    "        for j in range(len(sites_dict['site'])):\n",
    "            if sites_dict['site'] == site_name:\n",
    "                if not 'LP-DIR-T' in sites_dict['method_codes'][j]:\n",
    "                    sites_dict['method_codes'][j] += ':LP-DIR-T'\n",
    "elif treatment == 'AFD': \n",
    "    if not duplicate_site: \n",
    "        sites_dict['method_codes'].append('LP-DIR-AF')\n",
    "    else: \n",
    "        for j in range(len(sites_dict['site'])):\n",
    "            if sites_dict['site'] == site_name:\n",
    "                if not 'LP-DIR-AF' in sites_dict['method_codes'][j]:\n",
    "                    sites_dict['method_codes'][j] += ':LP-DIR-AF'\n",
    "else: print('Currently, only THD (thermal demag - directions) or AFD (AF demag - directions) are availabe options')\n",
    "\n",
    "if not site_dir_path.endswith('/'): site_dir_path += '/'\n",
    "for root, dirs, files in os.walk(site_dir_path): \n",
    "    for file in files:\n",
    "        with open(root+os.sep+file) as specimen_file:\n",
    "            measurement_data = specimen_file.readlines()\n",
    "        sequence, last_treat = 0, -1\n",
    "        for i in range(len(measurement_data)):\n",
    "            measurement_data[i] = measurement_data[i].strip('\\n')\n",
    "            if i == 0:\n",
    "                samples_dict['site'].append(site_name)\n",
    "                samples_dict['citations'].append('This study')\n",
    "                samples_dict['result_quality'].append('g')\n",
    "                specimens_dict['citations'].append('This study')\n",
    "                specimens_dict['result_quality'].append('g')\n",
    "                header = measurement_data[i].split()\n",
    "                name = header[0]\n",
    "                if treatment == 'THD': \n",
    "                    samples_dict['method_codes'].append('LP-DIR-T')\n",
    "                    specimens_dict['method_codes'].append('LP-DIR-T')\n",
    "                    specimens_dict['experiments'].append(name+'_'+'LP-DIR-T')\n",
    "                elif treatment == 'AFD': \n",
    "                    samples_dict['method_codes'].append('LP-DIR-AF')\n",
    "                    specimens_dict['method_codes'].append('LP-DIR-AF')\n",
    "                    specimens_dict['experiments'].append(name+'_'+'LP-DIR-AF')\n",
    "                samples_dict['sample'].append(name)\n",
    "                specimens_dict['sample'].append(name)\n",
    "                specimens_dict['specimen'].append(name)\n",
    "                bearing = header[1]\n",
    "                samples_dict['azimuth'].append(bearing)\n",
    "                plunge = header[2]\n",
    "                samples_dict['dip'].append(str(float(plunge)-90))\n",
    "                strike = header[3]\n",
    "                samples_dict['bed_dip_direction'].append(str(float(strike)+90))\n",
    "                dip = header[4]\n",
    "                samples_dict['bed_dip'].append(dip)\n",
    "                volume = header[5]\n",
    "                specimens_dict['volume'].append(str(float(volume)/10**6)) # m^3\n",
    "            else:\n",
    "                measurement = measurement_data[i].split()\n",
    "                measurement_dict['citations'].append('This study')\n",
    "                measurement_dict['specimen'].append(name)\n",
    "                treat = int(measurement[0])\n",
    "                if treatment == 'THD':\n",
    "                    measurement_dict['treat_temp'].append(str(float(treat)+273)) # K\n",
    "                    measurement_dict['meas_temp'].append(str(273)) # K\n",
    "                    measurement_dict['treat_ac_field'].append(str(0)) # T\n",
    "                    measurement_dict['method_codes'].append('LT-T-Z')\n",
    "                    measurement_dict['experiment'].append(name+'_'+'LP-DIR-T')\n",
    "                    measurement_dict['measurement'].append(name+'_'+'LP-DIR-T'+'-'+str(i))\n",
    "                    \n",
    "                elif treatment == 'AFD':\n",
    "                    measurement_dict['treat_temp'].append(str(273)) # K\n",
    "                    measurement_dict['meas_temp'].append(str(273)) # K\n",
    "                    measurement_dict['treat_ac_field'].append(str(float(treat)/10**3)) # T\n",
    "                    measurement_dict['method_codes'].append('LT-AF-Z')\n",
    "                    measurement_dict['experiment'].append(name+'_'+'LP-DIR-AF')\n",
    "                    measurement_dict['measurement'].append(name+'_'+'LP-DIR-AF'+'-'+str(i))\n",
    "\n",
    "                if treat > last_treat: sequence += 1\n",
    "                if sequence == 1:\n",
    "                    measurement_dict['quality'].append('g')\n",
    "                else:\n",
    "                        if not treat > last_treat: # attempting to catch repeat measurements\n",
    "                            measurement_dict['quality'][-1] = 'b' # assumes that the first one was bad\n",
    "                        measurement_dict['quality'].append('g')\n",
    "                    \n",
    "                if sequence < 10: seq_str = '0'+str(sequence)\n",
    "                else: seq_str = str(sequence)\n",
    "                measurement_dict['sequence'].append(seq_str)\n",
    "                intensity = measurement[1] # mA/m\n",
    "                measurement_dict['magn_volume'].append(str(float(intensity)/10**3)) # A/m\n",
    "                measurement_dict['magn_moment'].append(str((float(intensity)/10**3)*(float(volume)/10**6))) # Am^2\n",
    "                GDec = (measurement[2]) # not needed for demag GUI\n",
    "                GInc = (measurement[3]) # not needed for demag GUI\n",
    "                a95 = measurement[4]\n",
    "                measurement_dict['dir_csd'].append(a95)\n",
    "                CDec = measurement[5]\n",
    "                measurement_dict['dir_dec'].append(CDec)\n",
    "                CInc = measurement[6]\n",
    "                measurement_dict['dir_inc'].append(CInc)\n",
    "                measurement_dict['instrument_codes'].append('Molspin')\n",
    "                measurement_dict['treat_dc_field'].append(0)\n",
    "                measurement_dict['treat_dc_field_theta'].append(0)\n",
    "                measurement_dict['treat_dc_field_phi'].append(0)\n",
    "                measurement_dict['treat_step_num'].append(seq_str)\n",
    "\n",
    "print('Specimens for site', site_name, 'added successfully')\n",
    "#for key in measurement_dict.keys():\n",
    "#    print(key,len(measurement_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcc43b",
   "metadata": {},
   "source": [
    "## JR6 data\n",
    "\n",
    "convert jr6 data to magic format. Experimental. Agico orientation parameters are not considered at the moment. Use with caution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d74f32e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "file = 'clinker_demag_batch6.jr6' #PI-PTSD-pottery-pilot.jr6'\n",
    "volume = 7 #cc\n",
    "\n",
    "protocol = 'Dir' # 'PTSD'\n",
    "# for pseuro-Thellier and Shaw experiments: ARM demag series have to have .01 as identifier in AF field\n",
    "\n",
    "DC_field = 60 # uT value for the used lab field in intensity experiments\n",
    "DC_phi = 0 # 'declination' of applied field \n",
    "DC_theta = 90 # 'inclination' of applied field \n",
    "\n",
    "site_delimiter = '' # e.g., '-' for MAT1-1.2 ([site]-[sample].[specimen]); if there is none, use ''\n",
    "site_chars = 2 # number of characters at the beginning of the sample name that are the site identifier. only used if \n",
    "               # site_delimiter = ''\n",
    "specimen_delimiter = '' # e.g., '.' for MAT1-1.2 ([site]-[sample].[specimen]); if specimen = sample, use ''\n",
    "specimen_chars = 1 # number of characters from the end of the sample names if there is no delimiter (use 0 for spec = sample)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "if not path.endswith('/'): path += '/'\n",
    "f=open(path+file)\n",
    "tempdata=f.readlines()\n",
    "f.close()\n",
    "tempfile = open('temp.txt','w')\n",
    "tempfile.close()\n",
    "for line in tempdata:\n",
    "    line = line[:9].strip(' ') + line[9:].replace('-',',-').replace(' ',',').replace('\\t',',')\n",
    "    while ',,' in line:\n",
    "        line=line.replace(',,',',')\n",
    "    with open('temp.txt','a') as tempfile:\n",
    "        tempfile.write(line)\n",
    "jr6_raw = pd.read_csv('temp.txt',sep=\",\",header=None,names=['specimen','treat','m_x','m_y','m_z','exp','azimuth','spec_plunge',\n",
    "                                                            'tect_azi','tect_plunge','bed_dip_direction','bed_plunge','orient_1',\n",
    "                                                            'orient_2','orient_3','orient_4','csd'])\n",
    "os.remove('temp.txt')\n",
    "jr6_raw['m_x'] = (jr6_raw['m_x'].astype(str)+'E'+jr6_raw['exp'].astype(str)).astype(float)\n",
    "jr6_raw['m_y'] = (jr6_raw['m_y'].astype(str)+'E'+jr6_raw['exp'].astype(str)).astype(float)\n",
    "jr6_raw['m_z'] = (jr6_raw['m_z'].astype(str)+'E'+jr6_raw['exp'].astype(str)).astype(float)\n",
    "jr6_raw['magn_volume'] = np.sqrt(jr6_raw['m_x']**2 + jr6_raw['m_x']**2 + jr6_raw['m_x']**2) # A/m\n",
    "jr6_raw['magn_moment'] = jr6_raw['magn_volume']*(float(volume)/10**6) # Am2\n",
    "jr6_raw['dir_dec'] = round((np.arctan2(jr6_raw['m_y'],jr6_raw['m_x'])*180/np.pi)%360,2)\n",
    "jr6_raw['dir_inc'] = round((np.arctan2(jr6_raw['m_z'],np.sqrt(jr6_raw['m_x']**2+jr6_raw['m_y']**2)) *180/np.pi),2) # A/m\n",
    "if sum(jr6_raw['tect_azi'] == 0): del jr6_raw['tect_azi']\n",
    "if sum(jr6_raw['tect_plunge'] == 0): del jr6_raw['tect_plunge']\n",
    "jr6_raw['dir_csd'] = np.zeros(len(jr6_raw))\n",
    "jr6_raw['meas_temp'] = np.ones(len(jr6_raw))*273\n",
    "jr6_raw['volume'] = np.ones(len(jr6_raw))*float(volume)/10**6\n",
    "jr6_raw['dip'] = jr6_raw['azimuth']-90\n",
    "jr6_raw['instrument_codes'] = 'JR6'\n",
    "jr6_raw['quality'] = 'g'\n",
    "jr6_raw['result_quality'] = 'g'\n",
    "jr6_raw['citations'] = 'This study'\n",
    "jr6_raw['bed_dip'] = jr6_raw['bed_plunge']-90\n",
    "jr6_raw['treat_dc_field'] = np.zeros(len(jr6_raw))#np.ones(len(jr6_raw)) * DC_field *10**-6\n",
    "jr6_raw['treat_dc_field_phi'] = np.zeros(len(jr6_raw))#np.ones(len(jr6_raw)) * DC_phi\n",
    "jr6_raw['treat_dc_field_theta'] = np.zeros(len(jr6_raw))#np.ones(len(jr6_raw)) * DC_theta\n",
    "\n",
    "jr6_raw['index'] = jr6_raw.index\n",
    "jr6_raw = jr6_raw.sort_values(by=['specimen','index'])\n",
    "\n",
    "site, sample, method_code, treat_ac_field, experiment,treat_temp, measurement = [],[],[],[],[],[],[]\n",
    "sequence, treat_step_num = [],[]\n",
    "lastname = ''\n",
    "for i in range(len(jr6_raw)):\n",
    "    if i == 0: lastname = jr6_raw.loc[jr6_raw.index[i],'specimen']\n",
    "    if not site_delimiter == '': \n",
    "        site.append(jr6_raw.loc[jr6_raw.index[i],'specimen'].split(site_delimiter)[0])\n",
    "    else: \n",
    "        site.append(jr6_raw.loc[jr6_raw.index[i],'specimen'][:site_chars+1])\n",
    "                       \n",
    "    if not specimen_delimiter == '': sample.append(''.join(jr6_raw.loc[jr6_raw.index[i],'specimen'].split(site_delimiter)[:-1]))\n",
    "    else:\n",
    "        if not specimen_chars == 0:\n",
    "            sample.append(jr6_raw.loc[jr6_raw.index[i],'specimen'][:-specimen_chars])\n",
    "        else:\n",
    "            sample.append(jr6_raw.loc[jr6_raw.index[i],'specimen'])\n",
    "            \n",
    "    if i > 0 and lastname == jr6_raw.loc[jr6_raw.index[i],'specimen']:\n",
    "        sequence.append(int(sequence[-1])+1)\n",
    "    else:\n",
    "        sequence.append(0)\n",
    "    if sequence[-1] < 10: seq_str = '00'+str(sequence[-1])\n",
    "    elif i < 100: seq_str = '0'+str(sequence[-1])\n",
    "    else: seq_str = str(sequence[-1])\n",
    "    treat_step_num.append(int(seq_str))\n",
    "    sequence[-1] = seq_str\n",
    "                        \n",
    "    if protocol == 'Dir':\n",
    "        if 'NRM' in jr6_raw.loc[jr6_raw.index[i],'treat']:\n",
    "            method_code.append('LT-AF-Z') \n",
    "            experiment.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-DIR-AF') \n",
    "            measurement.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-DIR-AF'+'-'+str(i)) \n",
    "            treat_temp.append(273)\n",
    "            treat_ac_field.append(0)\n",
    "        if 'AD' in jr6_raw.loc[jr6_raw.index[i],'treat']: \n",
    "            method_code.append('LT-AF-Z')\n",
    "            experiment.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-DIR-AF')\n",
    "            measurement.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-DIR-AF'+'-'+str(i))\n",
    "            treat_temp.append(273)\n",
    "            treat_ac_field.append(float(jr6_raw.loc[jr6_raw.index[i],'treat'].strip('AD'))*10**-3)\n",
    "        elif 'TD' in jr6_raw.loc[jr6_raw.index[i],'treat']: \n",
    "            method_code.append('LT-T-Z')\n",
    "            experiment.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-DIR-T')\n",
    "            measurement.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-DIR-T'+'-'+str(i))\n",
    "            treat_temp.append(float(jr6_raw.loc[jr6_raw.index[i],'treat'].strip('TD'))+273)\n",
    "            treat_ac_field.append(0)\n",
    "            if 'NRM' in jr6_raw.loc[jr6_raw.index[i-1],'treat']:\n",
    "                method_code[-2] = 'LT-T-Z'\n",
    "                experiment[-2] = str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-DIR-T'\n",
    "                measurement[-2] = str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-DIR-T'+'-'+str(i)\n",
    "    elif protocol == 'PTSD':\n",
    "        if 'NRM' in jr6_raw.loc[jr6_raw.index[i],'treat']: \n",
    "            method_code.append('LT-NO:LP-DIR-AF:LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "            experiment.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "            measurement.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM'+'-'+str(i))\n",
    "            treat_temp.append(273)\n",
    "            treat_ac_field.append(0)\n",
    "        if 'AD' in jr6_raw.loc[jr6_raw.index[i],'treat']: \n",
    "            if not jr6_raw.loc[jr6_raw.index[i],'treat'].endswith('.01'):\n",
    "                method_code.append('LT-AF-Z:LP-DIR-AF:LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "                treat_temp.append(273)\n",
    "                treat_ac_field.append(float(jr6_raw.loc[jr6_raw.index[i],'treat'].strip('AD'))*10**-3)\n",
    "                experiment.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "                measurement.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM'+'-'+str(i))\n",
    "            else:\n",
    "                method_code.append('LT-AF-Z:LP-ARM-AFD:LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "                treat_temp.append(273)\n",
    "                treat_ac_field.append(float(jr6_raw.loc[jr6_raw.index[i],'treat'][:-1].strip('AD'))*10**-3)\n",
    "                experiment.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "                measurement.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM'+'-'+str(i))\n",
    "\n",
    "        elif 'TD' in jr6_raw.loc[jr6_raw.index[i],'treat']: \n",
    "            method_code.append('LT-T-I:LP-TRM-AFD:LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "            treat_temp.append(float(jr6_raw.loc[jr6_raw.index[i],'treat'].strip('TD'))+273)\n",
    "            treat_ac_field.append(0)\n",
    "            experiment.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "            measurement.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM'+'-'+str(i))\n",
    "        elif 'ARM' in jr6_raw.loc[jr6_raw.index[i],'treat']: \n",
    "            method_code.append('LT-AF-I:LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "            treat_temp.append(273)\n",
    "            treat_ac_field.append(float(jr6_raw.loc[jr6_raw.index[i],'treat'].strip('ARM'))*10**-3)\n",
    "            experiment.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM')\n",
    "            measurement.append(str(jr6_raw.loc[jr6_raw.index[i],'specimen'])+'_'+'LP-PI-TRM:LP-PI-ARM:LP-PI-ALT-AFARM'+'-'+str(i))\n",
    "    else:\n",
    "        print('protocol has not been added yet. contact whoever is responsible for this notebook')\n",
    "    lastname = jr6_raw.loc[jr6_raw.index[i],'specimen']    \n",
    "jr6_raw['site'] = site\n",
    "jr6_raw['sample'] = sample\n",
    "jr6_raw['method_codes'] = method_code\n",
    "jr6_raw['experiment'] = experiment\n",
    "jr6_raw['measurement'] = measurement\n",
    "jr6_raw['sequence'] = sequence\n",
    "jr6_raw['treat_step_num'] = treat_step_num\n",
    "jr6_raw['treat_temp'] = treat_temp\n",
    "jr6_raw['treat_ac_field'] = treat_ac_field\n",
    "                                                   \n",
    "sites_new = list(set(jr6_raw['site']))\n",
    "for i in range(len(sites_new)):\n",
    "    if not sites_new[i] in sites_dict['site']:\n",
    "        sites_dict['site'].append(sites_new[i])\n",
    "for i in range(len(sites_dict['site'])):\n",
    "    temp_df = jr6_raw[jr6_raw['site'] == sites_dict['site'][i]]\n",
    "    if not len(temp_df) == 0:\n",
    "        sites_dict['method_codes'].append(':'.join(list(set(temp_df['method_codes'].astype(str)))))\n",
    "        sites_dict['citations'].append(':'.join(list(set(temp_df['citations'].astype(str)))))\n",
    "samples_new = list(set(jr6_raw['sample']))\n",
    "for i in range(len(samples_new)):\n",
    "    if not samples_new[i] in samples_dict:\n",
    "        samples_dict['sample'].append(samples_new[i])\n",
    "for i in range(len(samples_dict['sample'])):\n",
    "    temp_df = jr6_raw[jr6_raw['sample'] == samples_dict['sample'][i]]\n",
    "    if not len(temp_df) == 0:\n",
    "        for key in samples_dict.keys():\n",
    "            if not key == 'sample':\n",
    "                samples_dict[key].append(':'.join(list(set(temp_df[key].astype(str)))))\n",
    "specimens_new = list(set(jr6_raw['specimen']))\n",
    "for i in range(len(specimens_new)):\n",
    "    if not specimens_new[i] in specimens_dict:\n",
    "        specimens_dict['specimen'].append(specimens_new[i])\n",
    "for i in range(len(specimens_dict['specimen'])):\n",
    "    temp_df = jr6_raw[jr6_raw['specimen'] == specimens_dict['specimen'][i]]\n",
    "    if not len(temp_df) == 0:\n",
    "        for key in specimens_dict.keys():\n",
    "            if key == 'experiments':\n",
    "                specimens_dict[key].append(':'.join(list(set(temp_df['experiment'].astype(str)))))\n",
    "            elif not key == 'specimen':\n",
    "                specimens_dict[key].append(':'.join(list(set(temp_df[key].astype(str)))))\n",
    "\n",
    "for key in measurement_dict.keys():\n",
    "    measurement_dict[key] = jr6_raw[key].values\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "979989a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HB6-11B' 'HB6-11B' 'HB6-11B' 'HB6-11B' 'HB6-11B' 'HB6-11B' 'HB6-11B'\n",
      " 'HB6-11B' 'HB6-11B' 'HB6-11B' 'HB6-11B' 'HB6-11B' 'HB6-11B' 'HB6-11B'\n",
      " 'HB6-11B' 'HB6-13A' 'HB6-13A' 'HB6-13A' 'HB6-13A' 'HB6-13A' 'HB6-13A'\n",
      " 'HB6-13A' 'HB6-13A' 'HB6-13A' 'HB6-13A' 'HB6-13A' 'HB6-13A' 'HB6-13A'\n",
      " 'HB6-13A' 'HB6-13A' 'HB6-9A' 'HB6-9A' 'HB6-9A' 'HB6-9A' 'HB6-9A' 'HB6-9A'\n",
      " 'HB6-9A' 'HB6-9A' 'HB6-9A' 'HB6-9A' 'HB6-9A' 'HB6-9A' 'HB6-9A' 'HB6-9A'\n",
      " 'HB6-9A' 'HB8-1A' 'HB8-1A' 'HB8-1A' 'HB8-1A' 'HB8-1A' 'HB8-1A' 'HB8-1A'\n",
      " 'HB8-1A' 'HB8-1A' 'HB8-1A' 'HB8-1A' 'HB8-1A' 'HB8-1A' 'HB8-1A' 'HB8-1A'\n",
      " 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-4A'\n",
      " 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-4A' 'HB8-5A'\n",
      " 'HB8-5A' 'HB8-5A' 'HB8-5A' 'HB8-5A' 'HB8-5A' 'HB8-5A' 'HB8-5A' 'HB8-5A'\n",
      " 'HB8-5A' 'HB8-5A' 'HB8-5A' 'HB8-5A' 'HB8-5A' 'HB8-5A' 'KK1-1B' 'KK1-1B'\n",
      " 'KK1-1B' 'KK1-1B' 'KK1-1B' 'KK1-1B' 'KK1-1B' 'KK1-1B' 'KK1-1B' 'KK1-1B'\n",
      " 'KK1-1B' 'KK1-1B' 'KK1-1B' 'KK1-1B' 'KK1-1B' 'KK1-4A' 'KK1-4A' 'KK1-4A'\n",
      " 'KK1-4A' 'KK1-4A' 'KK1-4A' 'KK1-4A' 'KK1-4A' 'KK1-4A' 'KK1-4A' 'KK1-4A'\n",
      " 'KK1-4A' 'KK1-4A' 'KK1-4A' 'KK1-4A' 'KK1-4A' 'KK1-6A' 'KK1-6A' 'KK1-6A'\n",
      " 'KK1-6A' 'KK1-6A' 'KK1-6A' 'KK1-6A' 'KK1-6A' 'KK1-6A' 'KK1-6A' 'KK1-6A'\n",
      " 'KK1-6A' 'KK1-6A' 'KK1-6A' 'KK1-6A' 'KK3-2B' 'KK3-2B' 'KK3-2B' 'KK3-2B'\n",
      " 'KK3-2B' 'KK3-2B' 'KK3-2B' 'KK3-2B' 'KK3-2B' 'KK3-2B' 'KK3-2B' 'KK3-2B'\n",
      " 'KK3-2B' 'KK3-2B' 'KK3-2B' 'KK3-2B' 'KK3-2B' 'KK3-3A' 'KK3-3A' 'KK3-3A'\n",
      " 'KK3-3A' 'KK3-3A' 'KK3-3A' 'KK3-3A' 'KK3-3A' 'KK3-3A' 'KK3-3A' 'KK3-3A'\n",
      " 'KK3-3A' 'KK3-3A' 'KK3-3A' 'KK3-3A' 'KK3-3A' 'KK3-3A' 'KK3-5A' 'KK3-5A'\n",
      " 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK3-5A'\n",
      " 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK3-5A' 'KK7-1A'\n",
      " 'KK7-1A' 'KK7-1A' 'KK7-1A' 'KK7-1A' 'KK7-1A' 'KK7-1A' 'KK7-1A' 'KK7-1A'\n",
      " 'KK7-1A' 'KK7-1A' 'KK7-1A' 'KK7-1A' 'KK7-1A' 'KK7-1A' 'KK7-3B' 'KK7-3B'\n",
      " 'KK7-3B' 'KK7-3B' 'KK7-3B' 'KK7-3B' 'KK7-3B' 'KK7-3B' 'KK7-3B' 'KK7-3B'\n",
      " 'KK7-3B' 'KK7-3B' 'KK7-3B' 'KK7-3B' 'KK7-3B' 'KK7-7A' 'KK7-7A' 'KK7-7A'\n",
      " 'KK7-7A' 'KK7-7A' 'KK7-7A' 'KK7-7A' 'KK7-7A' 'KK7-7A' 'KK7-7A' 'KK7-7A'\n",
      " 'KK7-7A' 'KK7-7A' 'KK7-7A' 'KK7-7A'] 232\n",
      "['LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-AF-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-AF-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-AF-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'\n",
      " 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z' 'LT-T-Z'] 232\n",
      "[230.29 228.83 231.28 231.31 230.05 229.89 232.93 232.88 234.1  231.99\n",
      " 230.97 223.99 213.37 211.2  207.12 208.31 206.87 211.1  209.81 211.67\n",
      " 211.16 212.56 210.11 211.58 215.7  211.61 201.57 194.45 194.79 192.33\n",
      " 215.94 218.59 215.73 217.15 216.05 219.86 219.91 219.44 218.74 219.72\n",
      " 217.59 200.96 196.35 196.45 194.1   55.83  56.89  61.67  62.12  60.06\n",
      "  65.51  65.7   66.32  63.36  66.8   68.02  71.88  73.94 124.3  142.83\n",
      " 356.62   0.46   8.32  11.53  10.55  14.09  16.91  17.93  20.67  19.86\n",
      "  19.24  20.65  32.48  16.5  171.26  21.96  21.84  25.98  26.81  27.19\n",
      "  27.33  34.11  30.1   29.5   30.36  31.47  31.68  30.68 179.15 115.48\n",
      " 161.52 169.01 172.94 169.1  170.28 169.73 170.24 169.12 171.14 168.33\n",
      " 168.98 169.97 178.14 165.9  169.85 190.08 232.21 168.56 172.73 168.81\n",
      " 169.97 169.48 169.55 168.11 170.14 169.97 171.99 165.64 165.75 168.\n",
      " 177.91 179.68 179.35 183.6  182.69 181.37 183.79 182.06 183.76 183.13\n",
      " 186.8  188.77 182.72 181.03 183.11 184.24 176.75 184.03 187.69 185.11\n",
      " 186.07 188.39 191.76 190.3  187.1  188.21 190.08 186.64 188.94 194.75\n",
      " 192.47 194.7   51.03 169.4  176.93 183.2  179.3  179.73 182.03 177.55\n",
      " 183.05 179.03 181.3  180.08 182.74 183.41 181.89 184.74 185.31 120.48\n",
      " 187.3  188.81 190.69 194.29 193.91 190.62 192.36 190.7  188.55 190.14\n",
      " 188.3  194.76 191.86 195.88 193.72 196.55  81.7    2.57 270.78 273.66\n",
      " 271.8  271.95 271.88 267.68 269.09 268.   270.46 255.17 187.84 196.29\n",
      " 190.37 205.99 168.75  86.32  87.18  87.91  88.31  83.03  99.42  98.73\n",
      "  92.48  96.23 114.9  179.46 182.03 182.48 180.61 183.56  91.96  94.76\n",
      "  92.74  94.73  95.19  96.26  95.11  95.71 100.3  121.33 178.96 179.26\n",
      " 203.44 187.68] 232\n",
      "[  5.77   4.47   3.95   2.84   4.04   2.76   1.83   1.44   1.89   0.44\n",
      "   0.53  10.28  13.35  11.18  -0.25   3.49   4.98   2.7    2.19   2.06\n",
      "   0.28   1.16  -0.44   0.3   -0.46  -0.2    8.78  12.97  13.86   6.83\n",
      "   5.69   4.11   3.     2.51   4.46   2.43   2.36   1.86   1.32   0.42\n",
      "   1.02  11.08  10.63   7.     2.86   8.19   6.44   3.14   3.16   1.86\n",
      "   1.79   3.53   1.92   2.32   3.19   0.77  -1.41   1.57  25.44  69.08\n",
      "  45.9   39.14  30.06  23.1   17.63  11.06   8.7    8.31   7.27   7.36\n",
      "   5.78   1.82  14.67 -29.88 -55.14  36.92  30.27  23.5   18.16  13.79\n",
      "   9.54   5.37   6.03   2.79  -1.41  -3.7    3.5   -6.75  44.26  15.96\n",
      "  56.75  56.84  56.47  57.    56.83  56.81  57.12  57.46  57.72  58.18\n",
      "  60.15  58.75  60.77  61.18  54.06 -59.13  57.47  58.57  58.48  58.32\n",
      "  58.17  58.65  59.1   59.25  59.87  60.09  62.23  62.54  67.11  66.46\n",
      "  61.11  56.31  55.95  55.38  55.54  56.04  54.98  55.05  55.26  56.06\n",
      "  56.35  57.81  59.13  63.36  64.08  58.31  36.59  37.19  36.82  37.03\n",
      "  37.45  36.3   35.48  37.23  34.92  34.96  35.41  34.83  32.91  34.86\n",
      "  35.36  30.4  -32.6   35.88  38.74  37.75  37.16  37.92  37.31  37.03\n",
      "  37.61  36.8   37.77  36.8   36.06  35.48  35.98  34.87  33.2  -53.31\n",
      "  33.95  36.38  37.08  35.49  35.97  34.93  37.    36.45  36.25  35.88\n",
      "  36.26  34.91  36.67  34.38  35.01  30.45 -63.28 -63.87 -45.   -47.11\n",
      " -47.97 -47.91 -50.04 -50.66 -50.92 -52.2  -53.88 -60.83 -58.75 -55.68\n",
      " -56.68 -43.08  57.74  40.46  41.03  42.01  42.43  43.58  46.13  47.33\n",
      "  47.48  49.4   57.52  56.56  55.77  55.57  55.9   61.22  48.02  49.78\n",
      "  49.72  48.94  52.07  52.79  52.73  52.87  54.76  62.62  66.25  62.11\n",
      "  70.93  70.24] 232\n",
      "[3.39481958e+00 3.49874263e+00 3.29089653e+00 3.27357603e+00\n",
      " 3.30821704e+00 3.23893501e+00 2.94448637e+00 2.90984536e+00\n",
      " 2.77128129e+00 2.78860180e+00 2.23088144e+00 3.98371686e-01\n",
      " 9.99393316e-02 6.14878037e-02 3.58534517e-02 6.49519053e+00\n",
      " 6.56447256e+00 6.28734443e+00 6.28734443e+00 6.14878037e+00\n",
      " 6.04485732e+00 5.75040868e+00 5.85433173e+00 5.66380614e+00\n",
      " 5.23079344e+00 4.27816549e+00 1.04269459e+00 1.73378286e-01\n",
      " 9.83804859e-02 5.23079344e-02 2.84056332e+00 2.73490823e+00\n",
      " 2.81458256e+00 2.74356848e+00 2.73144412e+00 2.53918648e+00\n",
      " 2.44565574e+00 2.43179933e+00 2.39889037e+00 2.18931222e+00\n",
      " 1.46011883e+00 2.56863135e-01 1.70087389e-01 1.23148812e-01\n",
      " 7.72494660e-02 7.43049796e-03 7.20533136e-03 5.24811395e-03\n",
      " 4.26084499e-03 4.00103737e-03 2.75396078e-03 2.42487113e-03\n",
      " 2.28630707e-03 2.49415316e-03 1.71473030e-03 1.05481894e-03\n",
      " 4.36476804e-04 2.26898656e-04 7.32657492e-05 2.07846097e-05\n",
      " 4.10496041e-03 4.27816549e-03 4.38208854e-03 4.41672956e-03\n",
      " 4.55529362e-03 4.55529362e-03 4.33012702e-03 4.17424245e-03\n",
      " 3.81051178e-03 2.87693639e-03 1.84117001e-03 1.02017793e-03\n",
      " 6.52983154e-04 1.77188798e-04 1.00285742e-04 4.38208854e-03\n",
      " 4.71117820e-03 4.79778074e-03 4.90170379e-03 5.02294734e-03\n",
      " 5.12687039e-03 4.88438328e-03 5.10954988e-03 5.26543446e-03\n",
      " 4.84974226e-03 3.24239911e-03 1.78228028e-03 8.17527981e-04\n",
      " 1.16220609e-04 5.99289579e-05 1.94855716e+00 1.91738024e+00\n",
      " 1.92950460e+00 1.80826104e+00 1.79960079e+00 1.51034830e+00\n",
      " 1.35965988e+00 1.33367912e+00 7.22265187e-01 5.28275496e-01\n",
      " 1.50342010e-01 7.04944679e-02 1.33194707e-02 1.17952660e-02\n",
      " 1.16047404e-02 1.70433799e+00 1.08772791e+00 1.63505596e+00\n",
      " 1.62985981e+00 1.56750598e+00 1.57616623e+00 1.35273168e+00\n",
      " 1.22109582e+00 1.20897146e+00 6.47787002e-01 4.79778074e-01\n",
      " 1.20550736e-01 5.07490887e-02 1.08426381e-02 9.04130522e-03\n",
      " 9.02398471e-03 3.08305044e+00 3.03108891e+00 3.03108891e+00\n",
      " 2.94448637e+00 2.89252485e+00 2.61539672e+00 2.40755062e+00\n",
      " 2.37290961e+00 1.45838678e+00 1.14834969e+00 3.70658873e-01\n",
      " 1.60214700e-01 2.89252485e-02 1.59175469e-02 1.61253930e-02\n",
      " 1.40469320e+00 8.10599778e-01 4.74581921e-01 2.94275432e-01\n",
      " 2.94795047e-01 2.20663273e-01 2.14601095e-01 2.10617378e-01\n",
      " 2.04555200e-01 2.01783919e-01 1.69567774e-01 1.39949705e-01\n",
      " 1.21070351e-01 1.04615869e-01 1.06521125e-01 5.28275496e-02\n",
      " 7.18801085e-04 8.33116438e-01 4.84974226e-01 3.31860935e-01\n",
      " 2.28284296e-01 2.24300580e-01 1.66276878e-01 1.61946751e-01\n",
      " 1.59175469e-01 1.52766881e-01 1.45492268e-01 1.27825350e-01\n",
      " 1.04789074e-01 9.02398471e-02 8.41776692e-02 8.34848489e-02\n",
      " 4.84974226e-02 3.56802466e-04 1.73031876e+00 1.06174715e+00\n",
      " 6.14878037e-01 3.67194771e-01 3.63730670e-01 2.37464166e-01\n",
      " 2.23607759e-01 2.21875708e-01 2.16506351e-01 2.12176224e-01\n",
      " 1.67316108e-01 1.26266504e-01 1.00632152e-01 8.64293353e-02\n",
      " 8.93738217e-02 3.67194771e-02 1.92257640e-04 3.86247330e+00\n",
      " 1.03923048e-01 4.50333210e-01 2.07846097e-01 2.25166605e-01\n",
      " 1.90525589e-01 2.25166605e-01 8.66025404e-02 1.73205081e-01\n",
      " 3.46410162e-02 6.23538291e-01 3.39481958e-01 2.66735824e-01\n",
      " 1.81692130e-01 1.11197662e-01 6.61643408e+01 7.96743371e+00\n",
      " 5.88897275e+00 4.15692194e+00 3.29089653e+00 1.22975607e+01\n",
      " 1.31635861e+01 1.14315353e+01 3.11769145e+00 6.75499815e+00\n",
      " 1.35099963e+01 5.52524208e+00 5.36935750e+00 5.19615242e+00\n",
      " 4.86706277e+00 3.34285806e+00 1.90525589e-01 4.33012702e-01\n",
      " 2.42487113e-01 4.15692194e-01 3.98371686e-01 4.50333210e-01\n",
      " 3.63730670e-01 3.81051178e-01 5.88897275e-01 9.69948452e-01\n",
      " 1.90525589e-01 1.59868290e-01 8.34848489e-02 6.16610087e-02] 232\n",
      "['HB6-11B_LP-DIR-AF' 'HB6-11B_LP-DIR-T' 'HB6-11B_LP-DIR-T'\n",
      " 'HB6-11B_LP-DIR-T' 'HB6-11B_LP-DIR-T' 'HB6-11B_LP-DIR-T'\n",
      " 'HB6-11B_LP-DIR-T' 'HB6-11B_LP-DIR-T' 'HB6-11B_LP-DIR-T'\n",
      " 'HB6-11B_LP-DIR-T' 'HB6-11B_LP-DIR-T' 'HB6-11B_LP-DIR-T'\n",
      " 'HB6-11B_LP-DIR-T' 'HB6-11B_LP-DIR-T' 'HB6-11B_LP-DIR-T'\n",
      " 'HB6-13A_LP-DIR-AF' 'HB6-13A_LP-DIR-T' 'HB6-13A_LP-DIR-T'\n",
      " 'HB6-13A_LP-DIR-T' 'HB6-13A_LP-DIR-T' 'HB6-13A_LP-DIR-T'\n",
      " 'HB6-13A_LP-DIR-T' 'HB6-13A_LP-DIR-T' 'HB6-13A_LP-DIR-T'\n",
      " 'HB6-13A_LP-DIR-T' 'HB6-13A_LP-DIR-T' 'HB6-13A_LP-DIR-T'\n",
      " 'HB6-13A_LP-DIR-T' 'HB6-13A_LP-DIR-T' 'HB6-13A_LP-DIR-T'\n",
      " 'HB6-9A_LP-DIR-AF' 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T'\n",
      " 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T'\n",
      " 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T'\n",
      " 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T' 'HB6-9A_LP-DIR-T' 'HB8-1A_LP-DIR-AF'\n",
      " 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T'\n",
      " 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T'\n",
      " 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T'\n",
      " 'HB8-1A_LP-DIR-T' 'HB8-1A_LP-DIR-T' 'HB8-4A_LP-DIR-AF' 'HB8-4A_LP-DIR-T'\n",
      " 'HB8-4A_LP-DIR-T' 'HB8-4A_LP-DIR-T' 'HB8-4A_LP-DIR-T' 'HB8-4A_LP-DIR-T'\n",
      " 'HB8-4A_LP-DIR-T' 'HB8-4A_LP-DIR-T' 'HB8-4A_LP-DIR-T' 'HB8-4A_LP-DIR-T'\n",
      " 'HB8-4A_LP-DIR-T' 'HB8-4A_LP-DIR-T' 'HB8-4A_LP-DIR-T' 'HB8-4A_LP-DIR-T'\n",
      " 'HB8-4A_LP-DIR-T' 'HB8-5A_LP-DIR-AF' 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T'\n",
      " 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T'\n",
      " 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T'\n",
      " 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T' 'HB8-5A_LP-DIR-T'\n",
      " 'KK1-1B_LP-DIR-AF' 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T'\n",
      " 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T'\n",
      " 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T'\n",
      " 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T' 'KK1-1B_LP-DIR-T' 'KK1-4A_LP-DIR-AF'\n",
      " 'KK1-4A_LP-DIR-AF' 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T'\n",
      " 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T'\n",
      " 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T'\n",
      " 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T' 'KK1-4A_LP-DIR-T' 'KK1-6A_LP-DIR-AF'\n",
      " 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T'\n",
      " 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T'\n",
      " 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T'\n",
      " 'KK1-6A_LP-DIR-T' 'KK1-6A_LP-DIR-T' 'KK3-2B_LP-DIR-AF' 'KK3-2B_LP-DIR-T'\n",
      " 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T'\n",
      " 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T'\n",
      " 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T'\n",
      " 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T' 'KK3-2B_LP-DIR-T' 'KK3-3A_LP-DIR-AF'\n",
      " 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T'\n",
      " 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T'\n",
      " 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T'\n",
      " 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T' 'KK3-3A_LP-DIR-T'\n",
      " 'KK3-5A_LP-DIR-AF' 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T'\n",
      " 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T'\n",
      " 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T'\n",
      " 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T' 'KK3-5A_LP-DIR-T'\n",
      " 'KK3-5A_LP-DIR-T' 'KK7-1A_LP-DIR-AF' 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T'\n",
      " 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T'\n",
      " 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T'\n",
      " 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T' 'KK7-1A_LP-DIR-T'\n",
      " 'KK7-3B_LP-DIR-AF' 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T'\n",
      " 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T'\n",
      " 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T'\n",
      " 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T' 'KK7-3B_LP-DIR-T' 'KK7-7A_LP-DIR-AF'\n",
      " 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T'\n",
      " 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T'\n",
      " 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T'\n",
      " 'KK7-7A_LP-DIR-T' 'KK7-7A_LP-DIR-T'] 232\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 232\n",
      "[273. 373. 403. 418. 433. 448. 463. 478. 493. 573. 673. 773. 823. 843.\n",
      " 863. 273. 373. 403. 418. 433. 448. 463. 478. 493. 573. 673. 773. 823.\n",
      " 843. 863. 273. 373. 403. 418. 433. 448. 463. 478. 493. 573. 673. 773.\n",
      " 823. 843. 863. 273. 373. 403. 418. 433. 448. 463. 478. 493. 573. 673.\n",
      " 773. 823. 843. 863. 273. 373. 403. 418. 433. 448. 463. 478. 493. 573.\n",
      " 673. 773. 823. 843. 863. 273. 373. 403. 418. 433. 448. 463. 478. 493.\n",
      " 573. 673. 773. 823. 843. 863. 273. 373. 403. 418. 433. 448. 463. 478.\n",
      " 493. 573. 673. 773. 823. 843. 863. 273. 273. 373. 403. 418. 433. 448.\n",
      " 463. 478. 493. 573. 673. 773. 823. 843. 863. 273. 373. 403. 418. 433.\n",
      " 448. 463. 478. 493. 573. 673. 773. 823. 843. 863. 273. 373. 403. 418.\n",
      " 433. 448. 463. 478. 493. 573. 673. 773. 823. 843. 863. 903. 963. 273.\n",
      " 373. 403. 418. 433. 448. 463. 478. 493. 573. 673. 773. 823. 843. 863.\n",
      " 903. 963. 273. 373. 403. 418. 433. 448. 463. 478. 493. 573. 673. 773.\n",
      " 823. 843. 863. 903. 963. 273. 373. 403. 418. 433. 448. 463. 478. 493.\n",
      " 573. 673. 773. 823. 843. 863. 273. 373. 403. 418. 433. 448. 463. 478.\n",
      " 493. 573. 673. 773. 823. 843. 863. 273. 373. 403. 418. 433. 448. 463.\n",
      " 478. 493. 573. 673. 773. 823. 843. 863.] 232\n",
      "[273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273. 273.\n",
      " 273. 273. 273. 273. 273. 273. 273. 273.] 232\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0] 232\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 232\n",
      "['This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study' 'This study' 'This study' 'This study'\n",
      " 'This study' 'This study'] 232\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 232\n",
      "['JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6' 'JR6'\n",
      " 'JR6' 'JR6' 'JR6' 'JR6'] 232\n",
      "['000' '001' '002' '003' '004' '005' '006' '007' '008' '009' '010' '011'\n",
      " '012' '013' '014' '000' '001' '002' '003' '004' '005' '006' '007' '008'\n",
      " '009' '010' '011' '012' '013' '014' '000' '001' '002' '003' '004' '005'\n",
      " '006' '007' '008' '009' '010' '011' '012' '013' '014' '000' '001' '002'\n",
      " '003' '004' '005' '006' '007' '008' '009' '010' '011' '012' '013' '014'\n",
      " '000' '001' '002' '003' '004' '005' '006' '007' '008' '009' '010' '011'\n",
      " '012' '013' '014' '000' '001' '002' '003' '004' '005' '006' '007' '008'\n",
      " '009' '010' '011' '012' '013' '014' '000' '001' '002' '003' '004' '005'\n",
      " '006' '007' '008' '009' '10' '11' '12' '13' '14' '000' '001' '002' '003'\n",
      " '004' '005' '006' '007' '008' '009' '10' '11' '12' '13' '14' '15' '000'\n",
      " '001' '002' '003' '004' '005' '006' '007' '008' '009' '10' '11' '12' '13'\n",
      " '14' '000' '001' '002' '003' '004' '005' '006' '007' '008' '009' '10'\n",
      " '11' '12' '13' '14' '15' '16' '000' '001' '002' '003' '004' '005' '006'\n",
      " '007' '008' '009' '10' '11' '12' '13' '14' '15' '16' '000' '001' '002'\n",
      " '003' '004' '005' '006' '007' '008' '009' '10' '11' '12' '13' '14' '15'\n",
      " '16' '000' '001' '002' '003' '004' '005' '006' '007' '008' '009' '10'\n",
      " '11' '12' '13' '14' '000' '001' '002' '003' '004' '005' '006' '007' '008'\n",
      " '009' '10' '11' '12' '13' '14' '000' '001' '002' '003' '004' '005' '006'\n",
      " '007' '008' '009' '10' '11' '12' '13' '14'] 232\n",
      "[2.37637371e-05 2.44911984e-05 2.30362757e-05 2.29150322e-05\n",
      " 2.31575193e-05 2.26725451e-05 2.06114046e-05 2.03689175e-05\n",
      " 1.93989690e-05 1.95202126e-05 1.56161701e-05 2.78860180e-06\n",
      " 6.99575321e-07 4.30414626e-07 2.50974162e-07 4.54663337e-05\n",
      " 4.59513079e-05 4.40114110e-05 4.40114110e-05 4.30414626e-05\n",
      " 4.23140012e-05 4.02528608e-05 4.09803221e-05 3.96466430e-05\n",
      " 3.66155541e-05 2.99471585e-05 7.29886210e-06 1.21364800e-06\n",
      " 6.88663401e-07 3.66155541e-07 1.98839433e-05 1.91443576e-05\n",
      " 1.97020779e-05 1.92049794e-05 1.91201089e-05 1.77743054e-05\n",
      " 1.71195902e-05 1.70225953e-05 1.67922326e-05 1.53251855e-05\n",
      " 1.02208318e-05 1.79804194e-06 1.19061173e-06 8.62041687e-07\n",
      " 5.40746262e-07 5.20134858e-08 5.04373195e-08 3.67367976e-08\n",
      " 2.98259149e-08 2.80072616e-08 1.92777255e-08 1.69740979e-08\n",
      " 1.60041495e-08 1.74590721e-08 1.20031121e-08 7.38373259e-09\n",
      " 3.05533762e-09 1.58829059e-09 5.12860244e-10 1.45492268e-10\n",
      " 2.87347229e-08 2.99471585e-08 3.06746198e-08 3.09171069e-08\n",
      " 3.18870554e-08 3.18870554e-08 3.03108891e-08 2.92196971e-08\n",
      " 2.66735824e-08 2.01385547e-08 1.28881901e-08 7.14124548e-09\n",
      " 4.57088208e-09 1.24032158e-09 7.02000192e-10 3.06746198e-08\n",
      " 3.29782474e-08 3.35844652e-08 3.43119265e-08 3.51606314e-08\n",
      " 3.58880927e-08 3.41906829e-08 3.57668492e-08 3.68580412e-08\n",
      " 3.39481958e-08 2.26967938e-08 1.24759620e-08 5.72269587e-09\n",
      " 8.13544264e-10 4.19502706e-10 1.36399001e-05 1.34216617e-05\n",
      " 1.35065322e-05 1.26578273e-05 1.25972055e-05 1.05724381e-05\n",
      " 9.51761919e-06 9.33575385e-06 5.05585631e-06 3.69792847e-06\n",
      " 1.05239407e-06 4.93461275e-07 9.32362950e-08 8.25668620e-08\n",
      " 8.12331829e-08 1.19303660e-05 7.61409535e-06 1.14453917e-05\n",
      " 1.14090187e-05 1.09725419e-05 1.10331636e-05 9.46912176e-06\n",
      " 8.54767074e-06 8.46280025e-06 4.53450901e-06 3.35844652e-06\n",
      " 8.43855153e-07 3.55243621e-07 7.58984664e-08 6.32891365e-08\n",
      " 6.31678930e-08 2.15813531e-05 2.12176224e-05 2.12176224e-05\n",
      " 2.06114046e-05 2.02476739e-05 1.83077770e-05 1.68528544e-05\n",
      " 1.66103672e-05 1.02087075e-05 8.03844780e-06 2.59461211e-06\n",
      " 1.12150290e-06 2.02476739e-07 1.11422828e-07 1.12877751e-07\n",
      " 9.83285243e-06 5.67419845e-06 3.32207345e-06 2.05992803e-06\n",
      " 2.06356533e-06 1.54464291e-06 1.50220767e-06 1.47432165e-06\n",
      " 1.43188640e-06 1.41248743e-06 1.18697442e-06 9.79647937e-07\n",
      " 8.47492460e-07 7.32311081e-07 7.45647873e-07 3.69792847e-07\n",
      " 5.03160760e-09 5.83181507e-06 3.39481958e-06 2.32302654e-06\n",
      " 1.59799008e-06 1.57010406e-06 1.16393814e-06 1.13362725e-06\n",
      " 1.11422828e-06 1.06936817e-06 1.01844587e-06 8.94777447e-07\n",
      " 7.33523517e-07 6.31678930e-07 5.89243685e-07 5.84393942e-07\n",
      " 3.39481958e-07 2.49761726e-09 1.21122313e-05 7.43223002e-06\n",
      " 4.30414626e-06 2.57036340e-06 2.54611469e-06 1.66224916e-06\n",
      " 1.56525431e-06 1.55312996e-06 1.51554446e-06 1.48523357e-06\n",
      " 1.17121276e-06 8.83865527e-07 7.04425063e-07 6.05005347e-07\n",
      " 6.25616752e-07 2.57036340e-07 1.34580348e-09 2.70373131e-05\n",
      " 7.27461339e-07 3.15233247e-06 1.45492268e-06 1.57616623e-06\n",
      " 1.33367912e-06 1.57616623e-06 6.06217783e-07 1.21243557e-06\n",
      " 2.42487113e-07 4.36476804e-06 2.37637371e-06 1.86715077e-06\n",
      " 1.27184491e-06 7.78383633e-07 4.63150386e-04 5.57720360e-05\n",
      " 4.12228092e-05 2.90984536e-05 2.30362757e-05 8.60829251e-05\n",
      " 9.21451030e-05 8.00207473e-05 2.18238402e-05 4.72849870e-05\n",
      " 9.45699741e-05 3.86766945e-05 3.75855025e-05 3.63730670e-05\n",
      " 3.40694394e-05 2.34000064e-05 1.33367912e-06 3.03108891e-06\n",
      " 1.69740979e-06 2.90984536e-06 2.78860180e-06 3.15233247e-06\n",
      " 2.54611469e-06 2.66735824e-06 4.12228092e-06 6.78963917e-06\n",
      " 1.33367912e-06 1.11907803e-06 5.84393942e-07 4.31627061e-07] 232\n",
      "['HB6-11B_LP-DIR-AF-0' 'HB6-11B_LP-DIR-T-1' 'HB6-11B_LP-DIR-T-2'\n",
      " 'HB6-11B_LP-DIR-T-3' 'HB6-11B_LP-DIR-T-4' 'HB6-11B_LP-DIR-T-5'\n",
      " 'HB6-11B_LP-DIR-T-6' 'HB6-11B_LP-DIR-T-7' 'HB6-11B_LP-DIR-T-8'\n",
      " 'HB6-11B_LP-DIR-T-9' 'HB6-11B_LP-DIR-T-10' 'HB6-11B_LP-DIR-T-11'\n",
      " 'HB6-11B_LP-DIR-T-12' 'HB6-11B_LP-DIR-T-13' 'HB6-11B_LP-DIR-T-14'\n",
      " 'HB6-13A_LP-DIR-AF-15' 'HB6-13A_LP-DIR-T-16' 'HB6-13A_LP-DIR-T-17'\n",
      " 'HB6-13A_LP-DIR-T-18' 'HB6-13A_LP-DIR-T-19' 'HB6-13A_LP-DIR-T-20'\n",
      " 'HB6-13A_LP-DIR-T-21' 'HB6-13A_LP-DIR-T-22' 'HB6-13A_LP-DIR-T-23'\n",
      " 'HB6-13A_LP-DIR-T-24' 'HB6-13A_LP-DIR-T-25' 'HB6-13A_LP-DIR-T-26'\n",
      " 'HB6-13A_LP-DIR-T-27' 'HB6-13A_LP-DIR-T-28' 'HB6-13A_LP-DIR-T-29'\n",
      " 'HB6-9A_LP-DIR-AF-30' 'HB6-9A_LP-DIR-T-31' 'HB6-9A_LP-DIR-T-32'\n",
      " 'HB6-9A_LP-DIR-T-33' 'HB6-9A_LP-DIR-T-34' 'HB6-9A_LP-DIR-T-35'\n",
      " 'HB6-9A_LP-DIR-T-36' 'HB6-9A_LP-DIR-T-37' 'HB6-9A_LP-DIR-T-38'\n",
      " 'HB6-9A_LP-DIR-T-39' 'HB6-9A_LP-DIR-T-40' 'HB6-9A_LP-DIR-T-41'\n",
      " 'HB6-9A_LP-DIR-T-42' 'HB6-9A_LP-DIR-T-43' 'HB6-9A_LP-DIR-T-44'\n",
      " 'HB8-1A_LP-DIR-AF-45' 'HB8-1A_LP-DIR-T-46' 'HB8-1A_LP-DIR-T-47'\n",
      " 'HB8-1A_LP-DIR-T-48' 'HB8-1A_LP-DIR-T-49' 'HB8-1A_LP-DIR-T-50'\n",
      " 'HB8-1A_LP-DIR-T-51' 'HB8-1A_LP-DIR-T-52' 'HB8-1A_LP-DIR-T-53'\n",
      " 'HB8-1A_LP-DIR-T-54' 'HB8-1A_LP-DIR-T-55' 'HB8-1A_LP-DIR-T-56'\n",
      " 'HB8-1A_LP-DIR-T-57' 'HB8-1A_LP-DIR-T-58' 'HB8-1A_LP-DIR-T-59'\n",
      " 'HB8-4A_LP-DIR-AF-60' 'HB8-4A_LP-DIR-T-61' 'HB8-4A_LP-DIR-T-62'\n",
      " 'HB8-4A_LP-DIR-T-63' 'HB8-4A_LP-DIR-T-64' 'HB8-4A_LP-DIR-T-65'\n",
      " 'HB8-4A_LP-DIR-T-66' 'HB8-4A_LP-DIR-T-67' 'HB8-4A_LP-DIR-T-68'\n",
      " 'HB8-4A_LP-DIR-T-69' 'HB8-4A_LP-DIR-T-70' 'HB8-4A_LP-DIR-T-71'\n",
      " 'HB8-4A_LP-DIR-T-72' 'HB8-4A_LP-DIR-T-73' 'HB8-4A_LP-DIR-T-74'\n",
      " 'HB8-5A_LP-DIR-AF-75' 'HB8-5A_LP-DIR-T-76' 'HB8-5A_LP-DIR-T-77'\n",
      " 'HB8-5A_LP-DIR-T-78' 'HB8-5A_LP-DIR-T-79' 'HB8-5A_LP-DIR-T-80'\n",
      " 'HB8-5A_LP-DIR-T-81' 'HB8-5A_LP-DIR-T-82' 'HB8-5A_LP-DIR-T-83'\n",
      " 'HB8-5A_LP-DIR-T-84' 'HB8-5A_LP-DIR-T-85' 'HB8-5A_LP-DIR-T-86'\n",
      " 'HB8-5A_LP-DIR-T-87' 'HB8-5A_LP-DIR-T-88' 'HB8-5A_LP-DIR-T-89'\n",
      " 'KK1-1B_LP-DIR-AF-90' 'KK1-1B_LP-DIR-T-91' 'KK1-1B_LP-DIR-T-92'\n",
      " 'KK1-1B_LP-DIR-T-93' 'KK1-1B_LP-DIR-T-94' 'KK1-1B_LP-DIR-T-95'\n",
      " 'KK1-1B_LP-DIR-T-96' 'KK1-1B_LP-DIR-T-97' 'KK1-1B_LP-DIR-T-98'\n",
      " 'KK1-1B_LP-DIR-T-99' 'KK1-1B_LP-DIR-T-100' 'KK1-1B_LP-DIR-T-101'\n",
      " 'KK1-1B_LP-DIR-T-102' 'KK1-1B_LP-DIR-T-103' 'KK1-1B_LP-DIR-T-104'\n",
      " 'KK1-4A_LP-DIR-AF-105' 'KK1-4A_LP-DIR-AF-106' 'KK1-4A_LP-DIR-T-107'\n",
      " 'KK1-4A_LP-DIR-T-108' 'KK1-4A_LP-DIR-T-109' 'KK1-4A_LP-DIR-T-110'\n",
      " 'KK1-4A_LP-DIR-T-111' 'KK1-4A_LP-DIR-T-112' 'KK1-4A_LP-DIR-T-113'\n",
      " 'KK1-4A_LP-DIR-T-114' 'KK1-4A_LP-DIR-T-115' 'KK1-4A_LP-DIR-T-116'\n",
      " 'KK1-4A_LP-DIR-T-117' 'KK1-4A_LP-DIR-T-118' 'KK1-4A_LP-DIR-T-119'\n",
      " 'KK1-4A_LP-DIR-T-120' 'KK1-6A_LP-DIR-AF-121' 'KK1-6A_LP-DIR-T-122'\n",
      " 'KK1-6A_LP-DIR-T-123' 'KK1-6A_LP-DIR-T-124' 'KK1-6A_LP-DIR-T-125'\n",
      " 'KK1-6A_LP-DIR-T-126' 'KK1-6A_LP-DIR-T-127' 'KK1-6A_LP-DIR-T-128'\n",
      " 'KK1-6A_LP-DIR-T-129' 'KK1-6A_LP-DIR-T-130' 'KK1-6A_LP-DIR-T-131'\n",
      " 'KK1-6A_LP-DIR-T-132' 'KK1-6A_LP-DIR-T-133' 'KK1-6A_LP-DIR-T-134'\n",
      " 'KK1-6A_LP-DIR-T-135' 'KK3-2B_LP-DIR-AF-136' 'KK3-2B_LP-DIR-T-137'\n",
      " 'KK3-2B_LP-DIR-T-138' 'KK3-2B_LP-DIR-T-139' 'KK3-2B_LP-DIR-T-140'\n",
      " 'KK3-2B_LP-DIR-T-141' 'KK3-2B_LP-DIR-T-142' 'KK3-2B_LP-DIR-T-143'\n",
      " 'KK3-2B_LP-DIR-T-144' 'KK3-2B_LP-DIR-T-145' 'KK3-2B_LP-DIR-T-146'\n",
      " 'KK3-2B_LP-DIR-T-147' 'KK3-2B_LP-DIR-T-148' 'KK3-2B_LP-DIR-T-149'\n",
      " 'KK3-2B_LP-DIR-T-150' 'KK3-2B_LP-DIR-T-151' 'KK3-2B_LP-DIR-T-152'\n",
      " 'KK3-3A_LP-DIR-AF-153' 'KK3-3A_LP-DIR-T-154' 'KK3-3A_LP-DIR-T-155'\n",
      " 'KK3-3A_LP-DIR-T-156' 'KK3-3A_LP-DIR-T-157' 'KK3-3A_LP-DIR-T-158'\n",
      " 'KK3-3A_LP-DIR-T-159' 'KK3-3A_LP-DIR-T-160' 'KK3-3A_LP-DIR-T-161'\n",
      " 'KK3-3A_LP-DIR-T-162' 'KK3-3A_LP-DIR-T-163' 'KK3-3A_LP-DIR-T-164'\n",
      " 'KK3-3A_LP-DIR-T-165' 'KK3-3A_LP-DIR-T-166' 'KK3-3A_LP-DIR-T-167'\n",
      " 'KK3-3A_LP-DIR-T-168' 'KK3-3A_LP-DIR-T-169' 'KK3-5A_LP-DIR-AF-170'\n",
      " 'KK3-5A_LP-DIR-T-171' 'KK3-5A_LP-DIR-T-172' 'KK3-5A_LP-DIR-T-173'\n",
      " 'KK3-5A_LP-DIR-T-174' 'KK3-5A_LP-DIR-T-175' 'KK3-5A_LP-DIR-T-176'\n",
      " 'KK3-5A_LP-DIR-T-177' 'KK3-5A_LP-DIR-T-178' 'KK3-5A_LP-DIR-T-179'\n",
      " 'KK3-5A_LP-DIR-T-180' 'KK3-5A_LP-DIR-T-181' 'KK3-5A_LP-DIR-T-182'\n",
      " 'KK3-5A_LP-DIR-T-183' 'KK3-5A_LP-DIR-T-184' 'KK3-5A_LP-DIR-T-185'\n",
      " 'KK3-5A_LP-DIR-T-186' 'KK7-1A_LP-DIR-AF-187' 'KK7-1A_LP-DIR-T-188'\n",
      " 'KK7-1A_LP-DIR-T-189' 'KK7-1A_LP-DIR-T-190' 'KK7-1A_LP-DIR-T-191'\n",
      " 'KK7-1A_LP-DIR-T-192' 'KK7-1A_LP-DIR-T-193' 'KK7-1A_LP-DIR-T-194'\n",
      " 'KK7-1A_LP-DIR-T-195' 'KK7-1A_LP-DIR-T-196' 'KK7-1A_LP-DIR-T-197'\n",
      " 'KK7-1A_LP-DIR-T-198' 'KK7-1A_LP-DIR-T-199' 'KK7-1A_LP-DIR-T-200'\n",
      " 'KK7-1A_LP-DIR-T-201' 'KK7-3B_LP-DIR-AF-202' 'KK7-3B_LP-DIR-T-203'\n",
      " 'KK7-3B_LP-DIR-T-204' 'KK7-3B_LP-DIR-T-205' 'KK7-3B_LP-DIR-T-206'\n",
      " 'KK7-3B_LP-DIR-T-207' 'KK7-3B_LP-DIR-T-208' 'KK7-3B_LP-DIR-T-209'\n",
      " 'KK7-3B_LP-DIR-T-210' 'KK7-3B_LP-DIR-T-211' 'KK7-3B_LP-DIR-T-212'\n",
      " 'KK7-3B_LP-DIR-T-213' 'KK7-3B_LP-DIR-T-214' 'KK7-3B_LP-DIR-T-215'\n",
      " 'KK7-3B_LP-DIR-T-216' 'KK7-7A_LP-DIR-AF-217' 'KK7-7A_LP-DIR-T-218'\n",
      " 'KK7-7A_LP-DIR-T-219' 'KK7-7A_LP-DIR-T-220' 'KK7-7A_LP-DIR-T-221'\n",
      " 'KK7-7A_LP-DIR-T-222' 'KK7-7A_LP-DIR-T-223' 'KK7-7A_LP-DIR-T-224'\n",
      " 'KK7-7A_LP-DIR-T-225' 'KK7-7A_LP-DIR-T-226' 'KK7-7A_LP-DIR-T-227'\n",
      " 'KK7-7A_LP-DIR-T-228' 'KK7-7A_LP-DIR-T-229' 'KK7-7A_LP-DIR-T-230'\n",
      " 'KK7-7A_LP-DIR-T-231'] 232\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 232\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14  0  1  2  3  4  5  6  7  8\n",
      "  9 10 11 12 13 14  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14  0  1  2\n",
      "  3  4  5  6  7  8  9 10 11 12 13 14  0  1  2  3  4  5  6  7  8  9 10 11\n",
      " 12 13 14  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14  0  1  2  3  4  5\n",
      "  6  7  8  9 10 11 12 13 14  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14\n",
      " 15  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14  0  1  2  3  4  5  6  7\n",
      "  8  9 10 11 12 13 14 15 16  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14\n",
      " 15 16  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16  0  1  2  3  4\n",
      "  5  6  7  8  9 10 11 12 13 14  0  1  2  3  4  5  6  7  8  9 10 11 12 13\n",
      " 14  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] 232\n",
      "['g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'] 232\n"
     ]
    }
   ],
   "source": [
    "# this cell only exists to find issues with the notebook\n",
    "for key in measurement_dict.keys():\n",
    "    print(measurement_dict[key], len(measurement_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7f95d",
   "metadata": {},
   "source": [
    "### Write MagIC formatted files\n",
    "\n",
    "Run cell after all experiments of the current location have been added using the cell above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c3f470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files for demag GUI saved to C:\\Users\\dthal\\My Drive\\__Postdoc\\Misc\\Side_quests\\UF2MagIC\n"
     ]
    }
   ],
   "source": [
    "# save files for demag_gui\n",
    "\n",
    "with open('./sites.txt', 'w') as sites_file:\n",
    "    sites_file.write('tab\\tsites\\n')\n",
    "sites_df = pd.DataFrame(sites_dict)\n",
    "sites_df.to_csv('./sites.txt',mode='a',index=False,sep='\\t')\n",
    "    \n",
    "with open('./samples.txt', 'w') as samples_file:\n",
    "    samples_file.write('tab\\tsamples\\n')\n",
    "samples_df = pd.DataFrame(samples_dict)\n",
    "samples_df.to_csv('./samples.txt',mode='a',index=False,sep='\\t')\n",
    "\n",
    "with open('./specimens.txt', 'w') as specimens_file:\n",
    "    specimens_file.write('tab\\tspecimens\\n')\n",
    "specimens_df = pd.DataFrame(specimens_dict)\n",
    "specimens_df.to_csv('./specimens.txt',mode='a',index=False,sep='\\t')\n",
    "\n",
    "with open('./measurements.txt', 'w') as measurements_file:\n",
    "    measurements_file.write('tab\\tmeasurements\\n')\n",
    "measurement_df = pd.DataFrame(measurement_dict)\n",
    "measurements_df = measurement_df[(~measurement_df['specimen'].isna())]\n",
    "#measurement_df = measurement_df.sort_values(by=['specimen','sequence'])\n",
    "measurement_df.to_csv('./measurements.txt',mode='a',index=False,sep='\\t')\n",
    "\n",
    "print('Files for demag GUI saved to', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba0406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4596d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac6ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88006b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c6dd920",
   "metadata": {},
   "source": [
    "# Ignore everything below unless you know what you are doing!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "JR6 -> Liverpool format for Shaw/pseudo-Thellier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc8f40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS01-01A converted! \\(✿◠‿◠)/\n",
      "TS01-02A converted! \\(✿◠‿◠)/\n",
      "TS01-03A converted! \\(✿◠‿◠)/\n",
      "TS01-04A converted! \\(✿◠‿◠)/\n",
      "TS02-01A converted! \\(✿◠‿◠)/\n",
      "TS02-02A converted! \\(✿◠‿◠)/\n",
      "TS02-03A converted! \\(✿◠‿◠)/\n",
      "TS02-04A converted! \\(✿◠‿◠)/\n",
      "TS03-01A converted! \\(✿◠‿◠)/\n",
      "TS03-02A converted! \\(✿◠‿◠)/\n",
      "TS03-03A converted! \\(✿◠‿◠)/\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "file = 'PI-PTSD-pottery-pilot-edit.jr6'\n",
    "volume = 7 #cc\n",
    "\n",
    "meta = {\n",
    "    'section_code': 'TS',\n",
    "    'measurer': 'Daniele Thallner',\n",
    "    'density': 2.5,\n",
    "    'experiment': 'AFTH',\n",
    "    'bias': 60, #TRM bias field (uT)\n",
    "    'temp': 620, #TRM max temp (C)\n",
    "    'ARM_bias': 40, #ARM bias field (uT)\n",
    "    'comment': '',\n",
    "    'lat': 29.6, #GD: 45.9 , SC: 50, VT: 51, BB: 56, NW: 65.2\n",
    "    'lon': 278, #GD: 280.0, SC: 302, VT: 24, BB: 356, NW: 234\n",
    "    'ignore_chars': 0, #number of characters at the start of sample names that will be ignored\n",
    "    'step': 'NRM0',  # type of the first treatment in file (has to be in steps_all)\n",
    "    'steps_all': ['NRM0','ACQ0','ARM0','TRM1','ACQ1','ARM1','TRM2','ARM2'] # used protocol #,'ACQ0','ACQ1'\n",
    "    }   \n",
    "\n",
    "if not path.endswith('/'): path += '/'\n",
    "f = open(path+file)\n",
    "file = f.readlines()\n",
    "f.close()\n",
    "for line in file:\n",
    "    line = line[:9].strip(' ') + line[9:].replace('-',',-').replace(' ',',').replace('\\t',',')\n",
    "    while ',,' in line:\n",
    "        line=line.replace(',,',',')\n",
    "    with open('temp.txt','a') as tempfile:\n",
    "        tempfile.write(line)\n",
    "f = open('temp.txt')\n",
    "file = f.readlines()\n",
    "f.close()\n",
    "os.remove('temp.txt')\n",
    "samplelist = []\n",
    "for i in range(len(file)):\n",
    "    file[i] = file[i].split(',')\n",
    "    if not file[i][0] in samplelist: \n",
    "        samplelist.append(file[i][0])\n",
    "        with open('temp.txt','a') as tempfile:\n",
    "            tempfile.write(','.join(file[i]))\n",
    "        for j in range(i+1,len(file)):\n",
    "            temp = file[j].split(',')\n",
    "            if temp[0] == samplelist[-1]:\n",
    "                with open('temp.txt','a') as tempfile:\n",
    "                    tempfile.write(','.join(temp))\n",
    "f = open('temp.txt')\n",
    "file = f.readlines()\n",
    "f.close()\n",
    "os.remove('temp.txt')\n",
    "\n",
    "pthells = []\n",
    "for i in range(len(file)):\n",
    "    file[i] = file[i].strip('\\n').split(',')\n",
    "    if i == 0 or not file[i][0] == file[i-1][0]:\n",
    "        header, body, end = {},{'a':[]},{'a':'END'}\n",
    "        step, stepnr = 0, 0\n",
    "        if len(file[i][0]) > 9:\n",
    "                file[i][0] = file[i][0][-10:]\n",
    "        header['a'] = file[i][0][meta['ignore_chars']:]#+'.'+meta['steps_all'][step] #'sample_code'\n",
    "        header['b'] = 0 #'dip'\n",
    "        header['c'] = 90 #'yetn'\n",
    "        header['d'] = 0 #'height'\n",
    "        header['e'] = 0 #'position'\n",
    "        if np.isnan(header['e']) :\n",
    "            header['e'] = 0\n",
    "        header['f'] = 0 #thickness\n",
    "        header['g'] = 0 #unit dip\n",
    "        if np.isnan(header['g']) :\n",
    "            header['g'] = 0\n",
    "        header['h'] = 0 # unit dir\n",
    "        if np.isnan(header['h']) :\n",
    "            header['h'] = 360\n",
    "        header['i'] = meta['lat']#site lat\n",
    "        header['j'] = meta['lon']#site lon\n",
    "        if meta['step'][0] == 'T':\n",
    "            header['k'] = 'THAF-'+meta['experiment']\n",
    "        else: header['k'] = 'AF-D'   \n",
    "        header['l'] = ''+meta['measurer'] #analyst\n",
    "        header['m'] = 'JR6' #magnetometer\n",
    "        header['n'] = 'AF' #demagnetiser\n",
    "        header['o'] = '' #comment\n",
    "        if not type(header['o']) == str:\n",
    "            if np.isnan(header['o']):\n",
    "                header['o'] = 0 \n",
    "        header['p'] = '!!!NOT UPLOADED!!!'\n",
    "        header['q'] = '!!!NOT CONVERTED!!!'\n",
    "        header['r'] = volume #volume\n",
    "        header['s'] = meta['density']*1000 #density\n",
    "    if i >= 0:\n",
    "        if (len(body['a']) > 1 and not file[i][1][:2] == file[i-1][1][:2]) and not stepnr == 1:\n",
    "            step = step+1\n",
    "            stepnr = 0\n",
    "        #elif 'AD' in file[i][1]and 'AD' in file[i-1] and float(file[i][1].strip('AD')) < float(file[i-1][1].strip('AD')):\n",
    "        #    step = step+1 # if the file consists of more sets of AF measurements\n",
    "        #    stepnr = 0\n",
    "        #if len(file[i][-1]) < 10: print('ERROR! Check the jr6 file of', header['a'],'(probably fucked up import)')\n",
    "        \n",
    "        if 'NRM' in file[i][1]: tempfield = 0\n",
    "        elif 'AD' in file[i][1]: tempfield = int(float(file[i][1].strip('AD')))\n",
    "        elif 'ARM' in file[i][1]: tempfield = int(float(file[i][1].strip('ARM')))\n",
    "        elif 'TRM' in file[i][1]: tempfield = int(float(file[i][1].strip('TRM')))\n",
    "        else: tempfield = 0\n",
    "        try: body['a'].append(tempfield) # temperature/field\n",
    "        except: body['a'] = [tempfield]\n",
    "        \n",
    "        try: body['b'].append(0) # MW power\n",
    "        except: body['b'] = [0]\n",
    "        try: body['c'].append(0) # MW time\n",
    "        except: body['c'] = [0]\n",
    "        try: body['d'].append(float(file[i][2])*10**int(file[i][5])) #x (nAm2) A/m!\n",
    "        except: body['d'] = [float(file[i][2])*10**int(file[i][5])]\n",
    "        try: body['e'].append(float(file[i][3])*10**int(file[i][5])) #y (nAm2) A/m!\n",
    "        except: body['e'] = [float(file[i][3])*10**int(file[i][5])]\n",
    "        try: body['f'].append(float(file[i][4])*10**int(file[i][5])) #z (nAm2) A/m!\n",
    "        except: body['f'] = [float(file[i][4])*10**int(file[i][5])]\n",
    "        try: body['g'].append(float(header['r'])*float(header['s'])/1000) #mass\n",
    "        except: body['g'] = [float(header['r'])*float(header['s'])/1000]\n",
    "        try: body['h'].append(meta['ARM_bias']) #applied field intensity uT\n",
    "        except: body['h'] = [meta['ARM_bias']]\n",
    "        try: body['i'].append(0) #applied field dec\n",
    "        except: body['i'] = [0]\n",
    "        if body['h'][-1] > 0:\n",
    "            try: body['j'].append(90) #applied field inc\n",
    "            except: body['j'] = [90]\n",
    "        else:\n",
    "            try: body['j'].append(0) #applied field inc\n",
    "            except: body['j'] = [0]\n",
    "        datetime = ''#file[i][-1].split(' ')\n",
    "        try: body['k'].append(datetime) #measurement date\n",
    "        except: body['k'] = [datetime]\n",
    "        try: body['l'].append(datetime) #measurement time\n",
    "        except: body['l'] = [datetime] # if len(datetime[1]) < 0 ?\n",
    "        try: body['m'].append(meta['steps_all'][step]) #comment\n",
    "        except: \n",
    "            body['m'] = [meta['steps_all'][step]]\n",
    "        if 'TRM1' in meta['steps_all'][step] or 'TRM2' in meta['steps_all'][step]:\n",
    "            header['o'] = 'Shaw-TRM with '+ str(meta['bias']) +'uT at '+ str(meta['temp']) +'C'\n",
    "        elif 'ACQ0' in meta['steps_all'][step]:\n",
    "            header['o'] = 'pseudo-Thellier experiment'\n",
    "        if ('ACQ0' in meta['steps_all'][step] or 'ACQ1' in meta['steps_all'][step]) and not file[i][0] in pthells: pthells.append(file[i][0])\n",
    "        try: body['n'].append(stepnr) #step number\n",
    "        except: body['n'] = [stepnr]\n",
    "        stepnr+=1\n",
    "        if 'NRM' in file[i][1]: \n",
    "            try: body['o'].append('NRM') #steptype\n",
    "            except: body['o'] = ['NRM']\n",
    "        else:\n",
    "            if 'ARM' in file[i][1]:\n",
    "                try: body['o'].append('I') #steptype\n",
    "                except: body['o'] = ['I']\n",
    "            else:\n",
    "                try: body['o'].append('Z') #steptype\n",
    "                except: body['o'] = ['Z']\n",
    "                body['h'][-1] = 0\n",
    "        try: body['p'].append(0) #tristan gain\n",
    "        except: body['p'] = [0]\n",
    "        try: body['q'].append(0) #MW integral\n",
    "        except: body['q'] = [0]\n",
    "        try: body['r'].append(file[i][-1]) #jr6 error\n",
    "        except: body['r'] = [file[i][-1]]\n",
    "        try: body['s'].append(file[i][-1]) #Fit\n",
    "        except: body['s'] = [file[i][-1]]\n",
    "        try: body['t'].append(file[i][-1]) #Utrecht error\n",
    "        except: body['t'] = [file[i][-1]]\n",
    "        if 'TRM' in file[i][1]:\n",
    "            try: body['u'].append(0) #af peak field\n",
    "            except: body['u'] = [0]\n",
    "        else:\n",
    "            try: body['u'].append(tempfield) #af peak field\n",
    "            except: body['u'] = [tempfield]\n",
    "        if 'TRM' in file[i][1]:\n",
    "            try: body['v'].append(tempfield) #th peak field\n",
    "            except: body['v'] = [tempfield]\n",
    "        else:\n",
    "            try: body['v'].append(0) #th peak field\n",
    "            except: body['v'] = [0]\n",
    "                        #print(header['a'],file[i][0],body['m'][-1])\n",
    "        if len(body['a'])>1:\n",
    "            if body['a'][-1] == body['a'][-2] and body['m'][-1] == body['m'][-2]:\n",
    "                print('! double step found for sample',header['1'])\n",
    "                print(file[i])\n",
    "    if i < len(file)-1 or i == len(file)-1:\n",
    "        if i == len(file)-1 or not file[i][0] == file[i+1].split(',')[0]:\n",
    "            head = pd.DataFrame(header, index = [0])\n",
    "            head.loc[head.index[0], 'a'] = head.loc[head.index[0], 'a']\n",
    "            head.to_csv('./'+meta['section_code']+'_PTSD.csv', index = False, header=False, mode = 'a')\n",
    "            body_df = pd.DataFrame(body)\n",
    "            body_df.to_csv('./'+meta['section_code']+'_PTSD.csv', index = False, header=False, mode = 'a')\n",
    "            end_df = pd.DataFrame(end, index = [0])\n",
    "            end_df.to_csv('./'+meta['section_code']+'_PTSD.csv', index = False, header=False, mode = 'a')\n",
    "            print(header['a'] + ' converted! \\(✿◠‿◠)/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29161fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_residual(file,mode):\n",
    "    f = open(file)\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    h = open('./temp.csv',mode='a')\n",
    "    datalines = []\n",
    "    for line in lines:\n",
    "        line = line.strip(' ').split(',')\n",
    "        try: \n",
    "            float(line[20].strip(' '))\n",
    "            datalines.append(line)\n",
    "            if line[12] == mode:\n",
    "                nx = float(datalines[-1][3])\n",
    "                ny = float(datalines[-1][4])\n",
    "                nz = float(datalines[-1][5])\n",
    "                px, py, pz = 0,0,0\n",
    "            elif line[12] == 'ACQ'+str(mode[-1]) and px == 0 and py == 0 and pz == 0:\n",
    "                px = float(datalines[-1][3])\n",
    "                py = float(datalines[-1][4])\n",
    "                pz = float(datalines[-1][5])\n",
    "            elif line[12] == 'ARM' + str(mode[-1]):\n",
    "                ax = float(datalines[-1][3])\n",
    "                ay = float(datalines[-1][4])\n",
    "                az = float(datalines[-1][5])\n",
    "        except:\n",
    "            if not 'END' in line[0]:\n",
    "                templine = ''\n",
    "                for i in range(len(line)):\n",
    "                    templine += line[i].strip(' ') + ','\n",
    "                h.write(templine[:-1])\n",
    "                datalines = []\n",
    "            else:\n",
    "                for entry in datalines:\n",
    "                    if entry[12] == mode:\n",
    "                        entry[3] = float(entry[3]) - nx\n",
    "                        entry[4] = float(entry[4]) - ny\n",
    "                        entry[5] = float(entry[5]) - nz\n",
    "                    elif entry[12] == 'ACQ'+str(mode[-1]):\n",
    "                        entry[3] = float(entry[3]) - px\n",
    "                        entry[4] = float(entry[4]) - py\n",
    "                        entry[5] = float(entry[5]) - pz\n",
    "                    elif entry[12] == 'ARM' + str(mode[-1]):\n",
    "                        entry[3] = float(entry[3]) - ax \n",
    "                        entry[4] = float(entry[4]) - ay\n",
    "                        entry[5] = float(entry[5]) - az\n",
    "                    templine = ''\n",
    "                    for i in range(len(entry)):\n",
    "                        templine += str(entry[i]).strip(' ') + ','\n",
    "                    h.write(templine[:-1])\n",
    "    print('Residuals have been removed! (づ￣ ³￣)づ')\n",
    "    h.close()\n",
    "    \n",
    "def pTH_online(file, noresid = True,mode='NRM0'):\n",
    "    if noresid:\n",
    "        remove_residual(file,mode)\n",
    "        f = open('./temp.csv')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        os.remove('./temp.csv')\n",
    "        h = open('./'+file[:6]+'upload_noResidual.txt',mode='a')\n",
    "    else:\n",
    "        f = open(file)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        h = open('./'+file[:6]+'upload.txt',mode='a')\n",
    "    text = 'specimen\\tstep\\tdec_s\\tinc_s\\tmoment\\ttype\\n'\n",
    "    h.write(text)\n",
    "    #name,vol = '',0\n",
    "    name = ''\n",
    "    for line in lines:\n",
    "        text = ''\n",
    "        line = line.split(',')\n",
    "        try:\n",
    "            step = float(line[20])\n",
    "            if line[12] == mode or line[12] == 'ACQ'+str(mode[-1]) or line[12] == 'ARM' + str(mode[-1]):\n",
    "                #step = tmp/10\n",
    "                #if not float(line[3]) == 0 and not float(line[4]) == 0 and not float(line[5]) == 0:\n",
    "                try:\n",
    "                    if not float(line[3]) == 0 and not float(line[4]) == 0:\n",
    "                        dec = (np.arctan2(float(line[4]),float(line[3]))*180/np.pi)%360\n",
    "                    else:\n",
    "                        dec = tempdec\n",
    "                    #if((float(line[3]) < 0 and float(line[4]) > 0) or (float(line[3]) < 0 and float(line[4]) < 0)):\n",
    "                    #    dec = dec + 180\n",
    "                    #elif(float(line[3]) > 0 and float(line[4]) < 0):\n",
    "                    #    dec = dec + 360\n",
    "                    if not float(line[5]) == 0:\n",
    "                        inc = np.arctan2(float(line[5]),np.sqrt(float(line[3])**2+float(line[4])**2))*180/np.pi\n",
    "                    else:\n",
    "                        inc = tempinc\n",
    "                    moment = np.sqrt(float(line[3])**2+float(line[4])**2+float(line[5])**2)# * vol * 10**6 # emu/cc to nAm2 (convert PTSD already converts emu to nAm2)\n",
    "                    tempdec, tempinc = dec, inc\n",
    "                #else:\n",
    "                except:\n",
    "                    dec, inc, moment = tempdec,tempinc,0\n",
    "                    print(line)\n",
    "                if line[12] == mode:\n",
    "                    steptype = 0\n",
    "                elif line[12] == 'ACQ'+str(mode[-1]):\n",
    "                    steptype = 6\n",
    "                elif line[12] == 'ARM'+str(mode[-1]):\n",
    "                    steptype = 7\n",
    "                text += name + '\\t' + str(step) + '\\t' + str(dec) +'\\t' + str(inc) + '\\t' + str(moment) +'\\t' + str(steptype)+'\\n'\n",
    "        except:\n",
    "            if not 'END' in line[0]:\n",
    "                name = line[0].strip(' ')\n",
    "                #vol = float(line[17])\n",
    "        h.write(text)\n",
    "        text = ''\n",
    "    h.close()\n",
    "    print('╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ paleointensity.org - pseudoThellier-uploadfile (pth format) created!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "217216f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residuals have been removed! (づ￣ ³￣)づ\n",
      "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ paleointensity.org - pseudoThellier-uploadfile (pth format) created!\n"
     ]
    }
   ],
   "source": [
    "start = 'NRM0'\n",
    "\n",
    "if len(pthells) == 0: print('No specimens to convert')\n",
    "f = open('./'+meta['section_code']+'_PTSD.csv')\n",
    "ptsdfile = f.readlines()\n",
    "f.close()\n",
    "header,body, = {},{}\n",
    "    \n",
    "pth = False\n",
    "for line in ptsdfile:\n",
    "    line = line.split(',')\n",
    "    if line[0] in pthells:\n",
    "        pth = True\n",
    "        for i in range(len(line)):\n",
    "            header[str(i)] = line[i].strip('\\n')\n",
    "        head = pd.DataFrame(header, index = [0])\n",
    "        head.to_csv('./pth_temp.csv', index = False, header=False, mode = 'a')\n",
    "    elif 'END' in line[0]:\n",
    "        end_df.to_csv('./pth_temp.csv', index = False, header=False, mode = 'a')\n",
    "        pth = False\n",
    "    else:\n",
    "        if pth == True:\n",
    "            for i in range(len(line)):\n",
    "                body[str(i)] = line[i].strip('\\n')\n",
    "                if 'TRM' in line[12]: body['0'] = 0\n",
    "            body_df = pd.DataFrame(body, index = [0])\n",
    "            body_df.to_csv('./pth_temp.csv', index = False, header=False, mode = 'a')\n",
    "            \n",
    "pTH_online('./pth_temp.csv', noresid = True, mode=start)\n",
    "os.remove('./pth_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4c81a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TS01-01A', 'TS01-02A', 'TS01-03A', 'TS01-04A', 'TS02-01A', 'TS02-02A', 'TS02-03A', 'TS02-04A', 'TS03-01A', 'TS03-02A', 'TS03-03A']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b44541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
